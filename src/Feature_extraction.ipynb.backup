{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c37942e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0bbae8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: Dataset paths for gait analysis\n",
    "VIDEO_DATASET_PATH = r\"C:\\Users\\Josemiles\\Desktop\\Datasets\\After\\KOA-PD-NM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1165f536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MediaPipe pose estimation configuration\n",
    "POSE_LANDMARK_ENUM = mp.solutions.pose.PoseLandmark\n",
    "MEDIAPIPE_POSE = mp.solutions.pose\n",
    "\n",
    "# NumPy display configuration for cleaner output\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b8e74719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_joint_angle(point_a, point_b, point_c):\n",
    "    \"\"\"\n",
    "    Calculate joint angle between three points using dot product.\n",
    "    Robust implementation that prevents NaN/infinite values for timeseries data.\n",
    "    \n",
    "    Args:\n",
    "        point_a: First point coordinates [x, y]\n",
    "        point_b: Vertex point coordinates [x, y] \n",
    "        point_c: Third point coordinates [x, y]\n",
    "    \n",
    "    Returns:\n",
    "        Joint angle in degrees (always a valid finite number)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        point_a, point_b, point_c = np.array(point_a), np.array(point_b), np.array(point_c)\n",
    "        \n",
    "        # Check for invalid points (NaN, inf, or None)\n",
    "        if (np.any(np.isnan([point_a, point_b, point_c])) or \n",
    "            np.any(np.isinf([point_a, point_b, point_c]))):\n",
    "            return 90.0  # Default neutral angle for timeseries continuity\n",
    "        \n",
    "        vector_ba = point_a - point_b\n",
    "        vector_bc = point_c - point_b\n",
    "        \n",
    "        # Calculate norms with safety check\n",
    "        norm_ba = np.linalg.norm(vector_ba)\n",
    "        norm_bc = np.linalg.norm(vector_bc)\n",
    "        \n",
    "        # Prevent division by zero for timeseries stability\n",
    "        if norm_ba < 1e-8 or norm_bc < 1e-8:\n",
    "            return 90.0  # Default neutral angle\n",
    "        \n",
    "        # Calculate cosine with robust normalization\n",
    "        cos_angle = np.dot(vector_ba, vector_bc) / (norm_ba * norm_bc)\n",
    "        \n",
    "        # Ensure cos_angle is within valid range for arccos\n",
    "        cos_angle = np.clip(cos_angle, -1.0, 1.0)\n",
    "        \n",
    "        # Calculate angle and ensure it's finite\n",
    "        angle_radians = np.arccos(cos_angle)\n",
    "        angle_degrees = np.degrees(angle_radians)\n",
    "        \n",
    "        # Final safety check for timeseries integrity\n",
    "        if np.isnan(angle_degrees) or np.isinf(angle_degrees):\n",
    "            return 90.0\n",
    "            \n",
    "        return float(angle_degrees)\n",
    "        \n",
    "    except Exception:\n",
    "        # Fallback for any unexpected errors - maintain timeseries continuity\n",
    "        return 90.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9ee90066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_heel_strike_event(previous_heel_y, current_heel_y, velocity_threshold=0.005):\n",
    "    \"\"\"\n",
    "    Detect heel strike events based on vertical velocity threshold.\n",
    "    Robust implementation for timeseries data integrity.\n",
    "    \n",
    "    Args:\n",
    "        previous_heel_y: Previous frame heel Y coordinate\n",
    "        current_heel_y: Current frame heel Y coordinate  \n",
    "        velocity_threshold: Minimum velocity change to detect heel strike\n",
    "    \n",
    "    Returns:\n",
    "        Boolean indicating if heel strike occurred (never None/NaN)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle None or invalid values\n",
    "        if (previous_heel_y is None or current_heel_y is None or\n",
    "            np.isnan(previous_heel_y) or np.isnan(current_heel_y) or\n",
    "            np.isinf(previous_heel_y) or np.isinf(current_heel_y)):\n",
    "            return False\n",
    "        \n",
    "        # Convert to float for safety\n",
    "        prev_y = float(previous_heel_y)\n",
    "        curr_y = float(current_heel_y)\n",
    "        \n",
    "        # Calculate vertical velocity with bounds checking\n",
    "        vertical_velocity = curr_y - prev_y\n",
    "        \n",
    "        # Ensure finite velocity for timeseries consistency\n",
    "        if np.isnan(vertical_velocity) or np.isinf(vertical_velocity):\n",
    "            return False\n",
    "            \n",
    "        # Return heel strike detection\n",
    "        return -velocity_threshold < vertical_velocity < velocity_threshold\n",
    "        \n",
    "    except Exception:\n",
    "        # Fallback for any errors - maintain timeseries integrity\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "79aa5e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stride_length(previous_position, current_position):\n",
    "    \"\"\"\n",
    "    Calculate Euclidean distance between two heel positions.\n",
    "    Robust implementation for timeseries data integrity.\n",
    "    \n",
    "    Args:\n",
    "        previous_position: Previous heel position [x, y]\n",
    "        current_position: Current heel position [x, y]\n",
    "    \n",
    "    Returns:\n",
    "        Stride length as float (always valid, never NaN/inf)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if (previous_position is None or current_position is None or\n",
    "            len(previous_position) < 2 or len(current_position) < 2):\n",
    "            return 0.0\n",
    "        \n",
    "        prev_point = np.array(previous_position, dtype=float)\n",
    "        curr_point = np.array(current_position, dtype=float)\n",
    "        \n",
    "        # Check for invalid values\n",
    "        if (np.any(np.isnan(prev_point)) or np.any(np.isinf(prev_point)) or\n",
    "            np.any(np.isnan(curr_point)) or np.any(np.isinf(curr_point))):\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate distance with safety check\n",
    "        distance = np.linalg.norm(curr_point - prev_point)\n",
    "        \n",
    "        # Ensure finite result for timeseries continuity\n",
    "        if np.isnan(distance) or np.isinf(distance) or distance < 0:\n",
    "            return 0.0\n",
    "            \n",
    "        return float(distance)\n",
    "        \n",
    "    except Exception:\n",
    "        # Fallback for any errors - maintain timeseries integrity\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23223ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pose_landmarks_and_angles(mediapipe_landmarks):\n",
    "    \"\"\"\n",
    "    Extract pose landmarks coordinates and calculate joint angles.\n",
    "    \n",
    "    Args:\n",
    "        mediapipe_landmarks: MediaPipe pose landmarks object\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with landmark coordinates and calculated joint angles\n",
    "    \"\"\"\n",
    "    landmarks_data = {}\n",
    "    \n",
    "    # Extract all landmark coordinates\n",
    "    for landmark_idx, landmark_enum in enumerate(POSE_LANDMARK_ENUM):\n",
    "        landmark_name = landmark_enum.name\n",
    "        landmark = mediapipe_landmarks[landmark_idx]\n",
    "        \n",
    "        landmarks_data[f\"{landmark_name}_x\"] = float(landmark.x)\n",
    "        landmarks_data[f\"{landmark_name}_y\"] = float(landmark.y)\n",
    "        landmarks_data[f\"{landmark_name}_z\"] = float(landmark.z)\n",
    "        landmarks_data[f\"{landmark_name}_visibility\"] = float(landmark.visibility)\n",
    "\n",
    "    # Robust joint angle calculation with timeseries data integrity\n",
    "    def safe_get_landmark_coords(landmark_name_x, landmark_name_y, default_x=0.5, default_y=0.5):\n",
    "        \"\"\"Safely extract landmark coordinates with fallback values for timeseries continuity\"\"\"\n",
    "        x = landmarks_data.get(landmark_name_x, default_x)\n",
    "        y = landmarks_data.get(landmark_name_y, default_y)\n",
    "        \n",
    "        # Replace invalid values with defaults\n",
    "        if np.isnan(x) or np.isinf(x) or x is None:\n",
    "            x = default_x\n",
    "        if np.isnan(y) or np.isinf(y) or y is None:\n",
    "            y = default_y\n",
    "            \n",
    "        return [float(x), float(y)]\n",
    "    \n",
    "    # Calculate joint angles with robust error handling for timeseries\n",
    "    # Left knee angle (hip-knee-ankle)\n",
    "    left_hip_coords = safe_get_landmark_coords('LEFT_HIP_x', 'LEFT_HIP_y', 0.5, 0.5)\n",
    "    left_knee_coords = safe_get_landmark_coords('LEFT_KNEE_x', 'LEFT_KNEE_y', 0.45, 0.6)\n",
    "    left_ankle_coords = safe_get_landmark_coords('LEFT_ANKLE_x', 'LEFT_ANKLE_y', 0.4, 0.8)\n",
    "    landmarks_data['left_knee_angle'] = calculate_joint_angle(left_hip_coords, left_knee_coords, left_ankle_coords)\n",
    "    \n",
    "    # Right knee angle (hip-knee-ankle)\n",
    "    right_hip_coords = safe_get_landmark_coords('RIGHT_HIP_x', 'RIGHT_HIP_y', 0.5, 0.5)\n",
    "    right_knee_coords = safe_get_landmark_coords('RIGHT_KNEE_x', 'RIGHT_KNEE_y', 0.55, 0.6)\n",
    "    right_ankle_coords = safe_get_landmark_coords('RIGHT_ANKLE_x', 'RIGHT_ANKLE_y', 0.6, 0.8)\n",
    "    landmarks_data['right_knee_angle'] = calculate_joint_angle(right_hip_coords, right_knee_coords, right_ankle_coords)\n",
    "    \n",
    "    # Left hip angle (shoulder-hip-knee)\n",
    "    left_shoulder_coords = safe_get_landmark_coords('LEFT_SHOULDER_x', 'LEFT_SHOULDER_y', 0.45, 0.3)\n",
    "    landmarks_data['left_hip_angle'] = calculate_joint_angle(left_shoulder_coords, left_hip_coords, left_knee_coords)\n",
    "    \n",
    "    # Right hip angle (shoulder-hip-knee)\n",
    "    right_shoulder_coords = safe_get_landmark_coords('RIGHT_SHOULDER_x', 'RIGHT_SHOULDER_y', 0.55, 0.3)\n",
    "    landmarks_data['right_hip_angle'] = calculate_joint_angle(right_shoulder_coords, right_hip_coords, right_knee_coords)\n",
    "    \n",
    "    return landmarks_data\n",
    "def initialize_video_processing_pipeline(video_file_path):\n",
    "    \"\"\"\n",
    "    Initialize video capture and MediaPipe pose estimation.\n",
    "    \n",
    "    Args:\n",
    "        video_file_path: Path to video file\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (video_capture, fps, pose_detector)\n",
    "    \"\"\"\n",
    "    video_capture = cv2.VideoCapture(video_file_path)\n",
    "    frames_per_second = video_capture.get(cv2.CAP_PROP_FPS) or 30.0\n",
    "    \n",
    "    pose_detector = MEDIAPIPE_POSE.Pose(\n",
    "        static_image_mode=False, \n",
    "        min_detection_confidence=0.5, \n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "    \n",
    "    return video_capture, frames_per_second, pose_detector\n",
    "\n",
    "\n",
    "def process_video_for_gait_cycles(video_file_path, max_cycles_per_video=5):\n",
    "    video_capture, frames_per_second, pose_detector = initialize_video_processing_pipeline(video_file_path)\n",
    "    current_frame_number = 0\n",
    "    detected_gait_cycles = 0\n",
    "    is_recording_cycle = False\n",
    "    current_cycle_frames = []\n",
    "    previous_left_heel_y = None\n",
    "    previous_left_heel_position = None\n",
    "    extracted_frame_data = []\n",
    "    \n",
    "    # Get video metadata for temporal context\n",
    "    total_video_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_duration_seconds = total_video_frames / frames_per_second if frames_per_second > 0 else 0\n",
    "\n",
    "    while video_capture.isOpened() and detected_gait_cycles < max_cycles_per_video:\n",
    "        frame_read_success, video_frame = video_capture.read()\n",
    "        if not frame_read_success:\n",
    "            break\n",
    "<<<<<<< local\n",
    "        current_frame_number += 1\n",
    "        \n",
    "        # Calculate timestamp for current frame\n",
    "        frame_timestamp = current_frame_number / frames_per_second\n",
    "        \n",
    "        # Convert frame to RGB for MediaPipe processing\n",
    "        frame_rgb = cv2.cvtColor(video_frame, cv2.COLOR_BGR2RGB)\n",
    "        pose_results = pose_detector.process(frame_rgb)\n",
    "        \n",
    "        if pose_results.pose_landmarks:\n",
    "            pose_landmarks = pose_results.pose_landmarks.landmark\n",
    "            frame_data = extract_pose_landmarks_and_angles(pose_landmarks)\n",
    "            \n",
    "            # Add temporal metadata\n",
    "            frame_data['frame_index'] = current_frame_number\n",
    "            frame_data['timestamp_seconds'] = frame_timestamp\n",
    "            frame_data['frames_per_second'] = frames_per_second\n",
    "            frame_data['video_duration_seconds'] = video_duration_seconds\n",
    "            frame_data['total_video_frames'] = total_video_frames\n",
    "\n",
    "            current_left_heel_y = frame_data.get('LEFT_HEEL_y')\n",
    "            current_left_heel_position = [frame_data.get('LEFT_HEEL_x'), frame_data.get('LEFT_HEEL_y')]\n",
    "\n",
    "<<<<<<< local\n",
    "            if detect_heel_strike_event(previous_left_heel_y, current_left_heel_y):\n",
    "                if not is_recording_cycle:\n",
    "                    # Start new gait cycle\n",
    "                    is_recording_cycle = True\n",
    "                    current_cycle_frames = []\n",
    "                    cycle_start_frame = current_frame_number\n",
    "                    cycle_start_timestamp = frame_timestamp\n",
    "                    previous_left_heel_position = current_left_heel_position\n",
    "                else:\n",
    "                    # Complete current gait cycle\n",
    "                    is_recording_cycle = False\n",
    "                    detected_gait_cycles += 1\n",
    "                    cycle_end_frame = current_frame_number\n",
    "                    cycle_end_timestamp = frame_timestamp\n",
    "                    \n",
    "                    # Calculate cycle metrics\n",
    "                    stride_length = calculate_stride_length(previous_left_heel_position, current_left_heel_position)\n",
    "                    cycle_duration_seconds = cycle_end_timestamp - cycle_start_timestamp\n",
    "                    cycle_frame_count = cycle_end_frame - cycle_start_frame\n",
    "                    \n",
    "                    # Add cycle-level features to all frames in this cycle\n",
    "                    for cycle_frame_data in current_cycle_frames:\n",
    "                        cycle_frame_data['stride_length'] = stride_length\n",
    "                        cycle_frame_data['cycle_duration_seconds'] = cycle_duration_seconds\n",
    "                        cycle_frame_data['cycle_frame_count'] = cycle_frame_count\n",
    "                        cycle_frame_data['cycle_start_timestamp'] = cycle_start_timestamp\n",
    "                        cycle_frame_data['cycle_end_timestamp'] = cycle_end_timestamp\n",
    "                        \n",
    "                    extracted_frame_data.extend(current_cycle_frames)\n",
    "\n",
    "            previous_left_heel_y = current_left_heel_y\n",
    "\n",
    "            if is_recording_cycle:\n",
    "                current_cycle_frames.append(frame_data)\n",
    "\n",
    "    video_capture.release()\n",
    "    pose_detector.close()\n",
    "    return extracted_frame_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d022b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_video_temporal_features(video_dataframe):\n",
    "    \"\"\"\n",
    "    Calculate temporal gait features for a single video analysis.\n",
    "    \n",
    "    Creates summary statistics that capture temporal dynamics while maintaining \n",
    "    video-level coherence for musculoskeletal disorder classification.\n",
    "    \n",
    "    Args:\n",
    "        video_dataframe: DataFrame containing all frames from one video\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of calculated temporal features\n",
    "    \"\"\"\n",
    "    if len(video_dataframe) == 0:\n",
    "        return {}\n",
    "        \n",
    "    temporal_feature_dict = {}\n",
    "    \n",
    "    # Video-level temporal information\n",
    "    temporal_feature_dict['extracted_frames_count'] = len(video_dataframe)\n",
    "    temporal_feature_dict['video_duration_seconds'] = video_dataframe['video_duration_seconds'].iloc[0] if 'video_duration_seconds' in video_dataframe else 0\n",
    "    temporal_feature_dict['average_fps'] = video_dataframe['frames_per_second'].iloc[0] if 'frames_per_second' in video_dataframe else 30\n",
    "    \n",
    "    # Get actual total frames from video metadata\n",
    "    if 'total_video_frames' in video_dataframe and len(video_dataframe) > 0:\n",
    "        temporal_feature_dict['total_frames'] = int(video_dataframe['total_video_frames'].iloc[0])\n",
    "    else:\n",
    "        # Fallback: calculate from duration and fps\n",
    "        if temporal_feature_dict['video_duration_seconds'] > 0 and temporal_feature_dict['average_fps'] > 0:\n",
    "            temporal_feature_dict['total_frames'] = int(temporal_feature_dict['video_duration_seconds'] * temporal_feature_dict['average_fps'])\n",
    "        else:\n",
    "            temporal_feature_dict['total_frames'] = temporal_feature_dict['extracted_frames_count']\n",
    "    \n",
    "    # Calculate extraction efficiency\n",
    "    if temporal_feature_dict['total_frames'] > 0:\n",
    "        temporal_feature_dict['extraction_efficiency'] = temporal_feature_dict['extracted_frames_count'] / temporal_feature_dict['total_frames']\n",
    "    else:\n",
    "        temporal_feature_dict['extraction_efficiency'] = 0.0\n",
    "    \n",
    "    # Gait cycle statistics\n",
    "    if 'cycle_duration_seconds' in video_dataframe:\n",
    "        cycle_durations = video_dataframe['cycle_duration_seconds'].dropna()\n",
    "        if len(cycle_durations) > 0:\n",
    "            temporal_feature_dict['avg_cycle_duration'] = cycle_durations.mean()\n",
    "            temporal_feature_dict['std_cycle_duration'] = cycle_durations.std()\n",
    "            temporal_feature_dict['cadence_cycles_per_min'] = 60.0 / cycle_durations.mean() if cycle_durations.mean() > 0 else 0\n",
    "    \n",
    "    # Joint angle temporal dynamics (capture movement patterns)\n",
    "    angle_columns = [col for col in video_dataframe.columns if 'angle' in col]\n",
    "    for angle_col in angle_columns:\n",
    "        if angle_col in video_dataframe:\n",
    "            angles = video_dataframe[angle_col].dropna()\n",
    "            if len(angles) > 1:\n",
    "                temporal_feature_dict[f'{angle_col}_mean'] = angles.mean()\n",
    "                temporal_feature_dict[f'{angle_col}_std'] = angles.std()\n",
    "                temporal_feature_dict[f'{angle_col}_range'] = angles.max() - angles.min()\n",
    "                # Temporal derivative (rate of change)\n",
    "                angle_differences = angles.diff().dropna()\n",
    "                if len(angle_differences) > 0:\n",
    "                    temporal_feature_dict[f'{angle_col}_velocity_mean'] = angle_differences.mean()\n",
    "                    temporal_feature_dict[f'{angle_col}_velocity_std'] = angle_differences.std()\n",
    "    \n",
    "    # Stride characteristics\n",
    "    if 'stride_length' in video_dataframe:\n",
    "        strides = video_dataframe['stride_length'].dropna()\n",
    "        if len(strides) > 0:\n",
    "            temporal_feature_dict['avg_stride_length'] = strides.mean()\n",
    "            temporal_feature_dict['std_stride_length'] = strides.std()\n",
    "            temporal_feature_dict['stride_consistency'] = 1.0 - (strides.std() / strides.mean()) if strides.mean() > 0 else 0\n",
    "    \n",
    "    # Temporal symmetry (left vs right limb timing)\n",
    "    left_heel_y = video_dataframe.get('LEFT_HEEL_y', pd.Series())\n",
    "    right_heel_y = video_dataframe.get('RIGHT_HEEL_y', pd.Series())\n",
    "    if len(left_heel_y) > 1 and len(right_heel_y) > 1:\n",
    "        left_movement = left_heel_y.diff().dropna()\n",
    "        right_movement = right_heel_y.diff().dropna()\n",
    "        if len(left_movement) > 0 and len(right_movement) > 0:\n",
    "            # Cross-correlation for temporal symmetry\n",
    "            min_length = min(len(left_movement), len(right_movement))\n",
    "            left_normalized = (left_movement[:min_length] - left_movement[:min_length].mean()) / left_movement[:min_length].std()\n",
    "            right_normalized = (right_movement[:min_length] - right_movement[:min_length].mean()) / right_movement[:min_length].std()\n",
    "            temporal_feature_dict['limb_symmetry'] = np.corrcoef(left_normalized, right_normalized)[0, 1] if min_length > 1 else 0\n",
    "    \n",
    "    return temporal_feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afb6820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gait_analysis_features(video_dataset_paths, output_filename='gait_analysis_features.csv', \n",
    "                                  max_cycles_per_video=5, supported_video_extensions=None, \n",
    "                                  extraction_mode='aggregated', timeseries_sample_rate_hz=30):\n",
    "    SUPPORTED_VIDEO_EXTENSIONS = supported_video_extensions or ['.mp4', '.avi', '.mov', '.mkv']\n",
    "    \n",
    "    # Ensure video_dataset_paths is a list\n",
    "    if isinstance(video_dataset_paths, (str, Path)):\n",
    "        video_dataset_paths = [video_dataset_paths]\n",
    "\n",
    "    # Comprehensive video discovery\n",
    "    def find_all_videos_recursively(path, max_depth=10, current_depth=0):\n",
    "        all_videos = []\n",
    "        path = Path(path)\n",
    "        if not path.exists() or current_depth > max_depth:\n",
    "            return all_videos\n",
    "        if path.is_file() and path.suffix.lower() in SUPPORTED_VIDEO_EXTENSIONS:\n",
    "            return [path]\n",
    "        if path.is_dir():\n",
    "            direct_videos = [child for child in sorted(path.iterdir()) \n",
    "                           if child.is_file() and child.suffix.lower() in SUPPORTED_VIDEO_EXTENSIONS]\n",
    "            all_videos.extend(direct_videos)\n",
    "            for subdir in sorted([c for c in path.iterdir() if c.is_dir()]):\n",
    "                try:\n",
    "                    subdir_videos = find_all_videos_recursively(subdir, max_depth, current_depth + 1)\n",
    "                    all_videos.extend(subdir_videos)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error accessing {subdir}: {e}\")\n",
    "        return all_videos\n",
    "\n",
    "    # Discover all videos\n",
    "    all_video_files = []\n",
    "    for dataset_path in video_dataset_paths:\n",
    "        print(f\"Scanning directory tree: {dataset_path}\")\n",
    "        found_videos = find_all_videos_recursively(dataset_path)\n",
    "        all_video_files.extend(found_videos)\n",
    "        print(f\"Found {len(found_videos)} videos\")\n",
    "\n",
    "    if not all_video_files:\n",
    "        print('No videos found. Creating empty CSV.')\n",
    "        empty_dataframe = pd.DataFrame()\n",
    "        output_path = Path('../Datasets') / output_filename\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        empty_dataframe.to_csv(output_path, index=False)\n",
    "        return empty_dataframe\n",
    "\n",
    "    print(f\"\\n=== GAIT EXTRACTION ===\")\n",
    "    print(f\"Total videos: {len(all_video_files)}\")\n",
    "    print(f\"Mode: {extraction_mode.upper()}\")\n",
    "    if extraction_mode == 'timeseries':\n",
    "        print(f\"Sample rate: {timeseries_sample_rate_hz} Hz\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # Processing variables\n",
    "    all_extracted_data = []\n",
    "    global_time_offset = 0.0 if extraction_mode == 'timeseries' else None\n",
    "    video_sequence_id = 0\n",
    "    successful_extractions = 0\n",
    "    \n",
    "    for video_idx, video_path in enumerate(all_video_files):\n",
    "        video_sequence_id += 1\n",
    "        folder_name = video_path.parent.name or 'unknown'\n",
    "        \n",
    "        print(f\"\\n[VIDEO {video_idx+1}/{len(all_video_files)}] {video_path.name}\")\n",
    "        print(f\"Label: {folder_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Process video using the updated function\n",
    "            video_capture, frames_per_second, pose_detector = initialize_video_processing_pipeline(str(video_path))\n",
    "            total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            video_duration = total_frames / frames_per_second if frames_per_second > 0 else 0\n",
    "            \n",
    "            # Frame sampling for timeseries mode\n",
    "            frame_interval = max(1, int(frames_per_second / timeseries_sample_rate_hz)) if extraction_mode == 'timeseries' and frames_per_second > 0 else 1\n",
    "            \n",
    "            current_frame_number = 0\n",
    "            detected_gait_cycles = 0\n",
    "            is_recording_cycle = False\n",
    "            current_cycle_frames = []\n",
    "            previous_left_heel_y = None\n",
    "            previous_left_heel_position = None\n",
    "            video_frame_data = []\n",
    "            \n",
    "            while video_capture.isOpened() and detected_gait_cycles < max_cycles_per_video:\n",
    "                frame_read_success, video_frame = video_capture.read()\n",
    "                if not frame_read_success:\n",
    "                    break\n",
    "                current_frame_number += 1\n",
    "                \n",
    "                # Sample frames for timeseries\n",
    "                if extraction_mode == 'timeseries' and current_frame_number % frame_interval != 0:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate timestamps\n",
    "                local_timestamp = current_frame_number / frames_per_second\n",
    "                global_timestamp = (global_time_offset + local_timestamp) if extraction_mode == 'timeseries' else None\n",
    "                \n",
    "                frame_rgb = cv2.cvtColor(video_frame, cv2.COLOR_BGR2RGB)\n",
    "                pose_results = pose_detector.process(frame_rgb)\n",
    "                \n",
    "                if pose_results.pose_landmarks:\n",
    "                    pose_landmarks = pose_results.pose_landmarks.landmark\n",
    "                    frame_data = extract_pose_landmarks_and_angles(pose_landmarks)\n",
    "                    \n",
    "                    # Add metadata\n",
    "                    frame_data['video_id'] = video_path.stem\n",
    "                    frame_data['video_path'] = str(video_path)\n",
    "                    frame_data['disorder_label'] = folder_name\n",
    "                    frame_data['frame_index'] = current_frame_number\n",
    "                    frame_data['timestamp_seconds'] = local_timestamp\n",
    "                    frame_data['frames_per_second'] = frames_per_second\n",
    "                    frame_data['video_duration_seconds'] = video_duration\n",
    "                    frame_data['total_video_frames'] = total_frames\n",
    "                    \n",
    "                    # Timeseries-specific fields\n",
    "                    if extraction_mode == 'timeseries':\n",
    "                        frame_data['global_timeline'] = global_timestamp\n",
    "                        frame_data['video_sequence_id'] = video_sequence_id\n",
    "                        frame_data['sample_rate_hz'] = timeseries_sample_rate_hz\n",
    "                        frame_data['sampled_frame_idx'] = len(video_frame_data) + 1\n",
    "                    \n",
    "                    # Gait cycle detection\n",
    "                    current_left_heel_y = frame_data.get('LEFT_HEEL_y')\n",
    "                    current_left_heel_position = [frame_data.get('LEFT_HEEL_x'), frame_data.get('LEFT_HEEL_y')]\n",
    "                    \n",
    "                    if detect_heel_strike_event(previous_left_heel_y, current_left_heel_y):\n",
    "                        if not is_recording_cycle:\n",
    "                            is_recording_cycle = True\n",
    "                            current_cycle_frames = []\n",
    "                            cycle_start_frame = current_frame_number\n",
    "                            cycle_start_timestamp = local_timestamp\n",
    "                            previous_left_heel_position = current_left_heel_position\n",
    "                        else:\n",
    "                            is_recording_cycle = False\n",
    "                            detected_gait_cycles += 1\n",
    "                            cycle_end_timestamp = local_timestamp\n",
    "                            \n",
    "                            # Calculate cycle features\n",
    "                            stride_length = calculate_stride_length(previous_left_heel_position, current_left_heel_position)\n",
    "                            cycle_duration_seconds = cycle_end_timestamp - cycle_start_timestamp\n",
    "                            \n",
    "                            # Add cycle features to all frames in cycle\n",
    "                            for cycle_frame in current_cycle_frames:\n",
    "                                cycle_frame['gait_cycle_id'] = detected_gait_cycles\n",
    "                                cycle_frame['stride_length'] = stride_length\n",
    "                                cycle_frame['cycle_duration_seconds'] = cycle_duration_seconds\n",
    "                                cycle_frame['cycle_start_timestamp'] = cycle_start_timestamp\n",
    "                                cycle_frame['cycle_end_timestamp'] = cycle_end_timestamp\n",
    "                                if extraction_mode == 'timeseries':\n",
    "                                    cycle_frame['within_cycle_progress'] = (cycle_frame['timestamp_seconds'] - cycle_start_timestamp) / cycle_duration_seconds if cycle_duration_seconds > 0 else 0\n",
    "                            \n",
    "                            video_frame_data.extend(current_cycle_frames)\n",
    "                    \n",
    "                    previous_left_heel_y = current_left_heel_y\n",
    "                    if is_recording_cycle:\n",
    "                        current_cycle_frames.append(frame_data)\n",
    "            \n",
    "            video_capture.release()\n",
    "            pose_detector.close()\n",
    "            \n",
    "            if video_frame_data:\n",
    "                successful_extractions += 1\n",
    "                print(f\"  SUCCESS: Extracted {len(video_frame_data)} data points, {detected_gait_cycles} gait cycles\")\n",
    "                \n",
    "                if extraction_mode == 'aggregated':\n",
    "                    # Create video-level summary\n",
    "                    video_dataframe = pd.DataFrame(video_frame_data)\n",
    "                    temporal_feature_dict = calculate_video_temporal_features(video_dataframe)\n",
    "                    \n",
    "                    video_summary = {\n",
    "                        'video_id': video_path.stem,\n",
    "                        'video_path': str(video_path),\n",
    "                        'disorder_label': folder_name,\n",
    "                        'total_gait_cycles': detected_gait_cycles,\n",
    "                        **temporal_feature_dict\n",
    "                    }\n",
    "                    all_extracted_data.append(video_summary)\n",
    "                else:\n",
    "                    # Add frame-by-frame or timeseries data\n",
    "                    all_extracted_data.extend(video_frame_data)\n",
    "                \n",
    "                # Update global timeline for timeseries\n",
    "                if extraction_mode == 'timeseries':\n",
    "                    global_time_offset += video_duration + 1.0\n",
    "            else:\n",
    "                print(f\"  WARNING: No gait cycles detected\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create final DataFrame\n",
    "    result_dataframe = pd.DataFrame(all_extracted_data)\n",
    "    \n",
    "    # Validate and clean data for timeseries integrity\n",
    "    if len(result_dataframe) > 0:\n",
    "        # Check for any data quality issues\n",
    "        missing_count = result_dataframe.isnull().sum().sum()\n",
    "        if missing_count > 0:\n",
    "            print(f\"\\n  Found {missing_count} missing values - applying data cleaning...\")\n",
    "            result_dataframe = validate_and_clean_timeseries_data(result_dataframe)\n",
    "    \n",
    "    if len(result_dataframe) > 0 and extraction_mode == 'timeseries':\n",
    "        # Sort by global timeline and add sequence index\n",
    "        result_dataframe = result_dataframe.sort_values('global_timeline').reset_index(drop=True)\n",
    "        result_dataframe['timeseries_index'] = range(len(result_dataframe))\n",
    "        result_dataframe['time_delta'] = result_dataframe['global_timeline'].diff().fillna(0)\n",
    "        \n",
    "        # Add rolling window features with robust NaN handling for timeseries\n",
    "        window_size = min(10, len(result_dataframe))\n",
    "        for angle_col in ['left_knee_angle', 'right_knee_angle', 'left_hip_angle', 'right_hip_angle']:\n",
    "            if angle_col in result_dataframe.columns:\n",
    "                # Calculate rolling mean with forward/backward fill for timeseries continuity\n",
    "                rolling_mean = result_dataframe[angle_col].rolling(window=window_size, center=True, min_periods=1).mean()\n",
    "                rolling_std = result_dataframe[angle_col].rolling(window=window_size, center=True, min_periods=1).std()\n",
    "                \n",
    "                # Fill any remaining NaN values with column mean for timeseries integrity\n",
    "                column_mean = result_dataframe[angle_col].mean()\n",
    "                column_std = result_dataframe[angle_col].std()\n",
    "                \n",
    "                result_dataframe[f'{angle_col}_rolling_mean'] = rolling_mean.fillna(column_mean)\n",
    "                result_dataframe[f'{angle_col}_rolling_std'] = rolling_std.fillna(column_std if not np.isnan(column_std) else 0.0)\n",
    "    \n",
    "    # Save results\n",
    "    output_path = Path('../Datasets') / output_filename\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    result_dataframe.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n=== EXTRACTION COMPLETE ===\")\n",
    "    print(f\"Successfully processed: {successful_extractions}/{len(all_video_files)} videos\")\n",
    "    print(f\"Output mode: {extraction_mode.upper()}\")\n",
    "    print(f\"Saved {len(result_dataframe)} rows to {output_path}\")\n",
    "    \n",
    "    if 'disorder_label' in result_dataframe.columns and len(result_dataframe) > 0:\n",
    "        label_counts = result_dataframe['disorder_label'].value_counts()\n",
    "        print(f\"Disorder distribution:\")\n",
    "        for label, count in label_counts.items():\n",
    "            percentage = (count / len(result_dataframe)) * 100\n",
    "            print(f\"   - {label}: {count} samples ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(\"=\"*40)\n",
    "    return result_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ed587811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_display_data_quality(df, description=\"dataset\"):\n",
    "    \"\"\"Enhanced data quality checker with professional output formatting\"\"\"\n",
    "    print(f\"\\n=== DATA QUALITY REPORT FOR {description.upper()} ===\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    missing_count = missing_data.sum()\n",
    "    \n",
    "    if len(missing_data) > 0:\n",
    "        print(f\"WARNING: Found {missing_count} missing values - applying data cleaning...\")\n",
    "        \n",
    "        # Apply comprehensive data cleaning\n",
    "        cleaned_df = validate_and_clean_timeseries_data(df)\n",
    "        \n",
    "        # Recheck after cleaning\n",
    "        post_clean_missing = cleaned_df.isnull().sum().sum()\n",
    "        print(f\"INFO: After cleaning - Missing values reduced to: {post_clean_missing}\")\n",
    "        \n",
    "        return cleaned_df\n",
    "    else:\n",
    "        print(\"SUCCESS: No missing values found in original dataset\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f3529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_clean_timeseries_data(dataframe):\n",
    "  \n",
    "    print(\"\\\\n=== TIMESERIES DATA VALIDATION AND CLEANING ===\")\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    df = dataframe.copy()\n",
    "    \n",
    "    # 1. Check for missing values\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "    \n",
    "    if len(missing_data) > 0:\n",
    "        print(f\"ERROR: Found {len(missing_data)} columns with missing values:\")\n",
    "        for col, missing in missing_data.items():\n",
    "            total = len(df)\n",
    "            percentage = (missing / total) * 100\n",
    "            if missing > 0:\n",
    "                print(f\"ERROR: {col}: {missing} missing values ({percentage:.1f}%)\")\n",
    "            else:\n",
    "                print(f\"SUCCESS: {col}: No missing values\")\n",
    "    else:\n",
    "        print(\"SUCCESS: No missing values found in dataset!\")\n",
    "    \n",
    "    # 2. Check for infinite values\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    inf_data = {}\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        inf_count = np.isinf(df[col]).sum()\n",
    "        if inf_count > 0:\n",
    "            inf_data[col] = inf_count\n",
    "    \n",
    "    if inf_data:\n",
    "        for col, inf_count in inf_data.items():\n",
    "            print(f\"ERROR: {col}: {inf_count} infinite values\")\n",
    "    else:\n",
    "        print(\"SUCCESS: No infinite values found!\")\n",
    "    \n",
    "    # 3. Data cleaning operations\n",
    "    print(\"\\\\n=== APPLYING DATA CLEANING ===\")\n",
    "    \n",
    "    # Fill missing values with appropriate defaults\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['float64', 'float32', 'int64', 'int32']:\n",
    "            # For numeric columns, use forward fill, then backward fill, then 0\n",
    "            df[col] = df[col].fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
    "            \n",
    "            # Replace infinite values with column median\n",
    "            if np.isinf(df[col]).any():\n",
    "                median_val = df[col].replace([np.inf, -np.inf], np.nan).median()\n",
    "                if pd.isna(median_val):\n",
    "                    median_val = 0.0\n",
    "                df[col] = df[col].replace([np.inf, -np.inf], median_val)\n",
    "        \n",
    "        elif df[col].dtype == 'object':\n",
    "            # For string columns, fill with 'unknown'\n",
    "            df[col] = df[col].fillna('unknown')\n",
    "    \n",
    "    # 4. Final validation\n",
    "    final_missing = df.isnull().sum().sum()\n",
    "    final_inf = np.isinf(df.select_dtypes(include=[np.number])).sum().sum()\n",
    "    \n",
    "    if final_missing == 0 and final_inf == 0:\n",
    "        print(\"SUCCESS: Timeseries data is now clean and ready for analysis!\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"WARNING: Some data quality issues may remain\")\n",
    "        print(f\"Remaining missing values: {final_missing}\")\n",
    "        print(f\"Remaining infinite values: {final_inf}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5600139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_gait_analysis_pipeline(dataset_path, output_dir=\"../Datasets\", \n",
    "                                       max_cycles_per_video=5, timeseries_sample_rate_hz=10,\n",
    "                                       supported_extensions=None, validation_enabled=True):\n",
    "    \"\"\"\n",
    "    Complete gait analysis pipeline that orchestrates all processing steps in sequential order.\n",
    "    \n",
    "    This main function performs:\n",
    "    1. Video discovery and validation\n",
    "    2. Pose estimation and landmark extraction\n",
    "    3. Joint angle calculations with NaN prevention\n",
    "    4. Gait cycle detection and stride analysis\n",
    "    5. Temporal feature extraction\n",
    "    6. Data validation and cleaning\n",
    "    7. Timeseries generation with multi-level tracking\n",
    "    8. Professional output generation and reporting\n",
    "    \n",
    "    Args:\n",
    "        dataset_path (str): Path to video dataset directory\n",
    "        output_dir (str): Output directory for results\n",
    "        max_cycles_per_video (int): Maximum gait cycles to extract per video\n",
    "        timeseries_sample_rate_hz (int): Sampling rate for timeseries mode\n",
    "        supported_extensions (list): Video file extensions to process\n",
    "        validation_enabled (bool): Enable comprehensive data validation\n",
    "    \n",
    "    Returns:\n",
    "        dict: Complete analysis results with datasets and metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"         COMPLETE GAIT ANALYSIS PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Dataset Path: {dataset_path}\")\n",
    "    print(f\"Output Directory: {output_dir}\")\n",
    "    print(f\"Max Cycles per Video: {max_cycles_per_video}\")\n",
    "    print(f\"Timeseries Sample Rate: {timeseries_sample_rate_hz} Hz\")\n",
    "    print(f\"Data Validation: {'ENABLED' if validation_enabled else 'DISABLED'}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    pipeline_results = {\n",
    "        'aggregated_features': None,\n",
    "        'timeseries_features': None,\n",
    "        'validation_reports': [],\n",
    "        'processing_metrics': {},\n",
    "        'error_log': []\n",
    "    }\n",
    "    \n",
    "    # Step 1: Video Discovery and Validation\n",
    "    print(\"\\nSTEP 1: VIDEO DISCOVERY AND VALIDATION\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Set default extensions if not provided\n",
    "        if supported_extensions is None:\n",
    "            supported_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv']\n",
    "        \n",
    "        # Discover videos recursively\n",
    "        dataset_path_obj = Path(dataset_path)\n",
    "        if not dataset_path_obj.exists():\n",
    "            raise FileNotFoundError(f\"Dataset path does not exist: {dataset_path}\")\n",
    "        \n",
    "        print(f\"Scanning directory tree: {dataset_path}\")\n",
    "        all_videos = []\n",
    "        for ext in supported_extensions:\n",
    "            found_videos = list(dataset_path_obj.rglob(f\"*{ext}\"))\n",
    "            all_videos.extend(found_videos)\n",
    "        \n",
    "        print(f\"SUCCESS: Found {len(all_videos)} video files\")\n",
    "        pipeline_results['processing_metrics']['total_videos_found'] = len(all_videos)\n",
    "        \n",
    "        if len(all_videos) == 0:\n",
    "            print(\"WARNING: No videos found. Pipeline terminated.\")\n",
    "            return pipeline_results\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"ERROR in video discovery: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        pipeline_results['error_log'].append(error_msg)\n",
    "        return pipeline_results\n",
    "    \n",
    "    # Step 2: Aggregated Features Extraction\n",
    "    print(\"\\nSTEP 2: AGGREGATED FEATURES EXTRACTION\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        print(\"Extracting video-level aggregated features...\")\n",
    "        aggregated_df = extract_gait_analysis_features(\n",
    "            video_dataset_paths=dataset_path,\n",
    "            output_filename='aggregated_gait_features.csv',\n",
    "            max_cycles_per_video=max_cycles_per_video,\n",
    "            supported_video_extensions=supported_extensions,\n",
    "            extraction_mode='aggregated'\n",
    "        )\n",
    "        \n",
    "        if validation_enabled and len(aggregated_df) > 0:\n",
    "            print(\"Validating aggregated features...\")\n",
    "            aggregated_df = check_and_display_data_quality(aggregated_df, \"AGGREGATED FEATURES\")\n",
    "            pipeline_results['validation_reports'].append(\"Aggregated features validation completed\")\n",
    "        \n",
    "        pipeline_results['aggregated_features'] = aggregated_df\n",
    "        pipeline_results['processing_metrics']['aggregated_samples'] = len(aggregated_df)\n",
    "        print(f\"SUCCESS: Generated {len(aggregated_df)} aggregated feature samples\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"ERROR in aggregated extraction: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        pipeline_results['error_log'].append(error_msg)\n",
    "    \n",
    "    # Step 3: Timeseries Features Extraction\n",
    "    print(\"\\nSTEP 3: TIMESERIES FEATURES EXTRACTION\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        print(\"Extracting frame-by-frame timeseries features...\")\n",
    "        timeseries_df = extract_gait_analysis_features(\n",
    "            video_dataset_paths=dataset_path,\n",
    "            output_filename='timeseries_gait_features.csv',\n",
    "            max_cycles_per_video=max_cycles_per_video,\n",
    "            supported_video_extensions=supported_extensions,\n",
    "            extraction_mode='timeseries',\n",
    "            timeseries_sample_rate_hz=timeseries_sample_rate_hz\n",
    "        )\n",
    "        \n",
    "        if validation_enabled and len(timeseries_df) > 0:\n",
    "            print(\"Validating timeseries features...\")\n",
    "            timeseries_df = check_and_display_data_quality(timeseries_df, \"TIMESERIES FEATURES\")\n",
    "            pipeline_results['validation_reports'].append(\"Timeseries features validation completed\")\n",
    "        \n",
    "        pipeline_results['timeseries_features'] = timeseries_df\n",
    "        pipeline_results['processing_metrics']['timeseries_samples'] = len(timeseries_df)\n",
    "        print(f\"SUCCESS: Generated {len(timeseries_df)} timeseries feature samples\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"ERROR in timeseries extraction: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        pipeline_results['error_log'].append(error_msg)\n",
    "    \n",
    "    # Step 4: Advanced Data Analysis and Reporting\n",
    "    print(\"\\nSTEP 4: ADVANCED DATA ANALYSIS AND REPORTING\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Analyze disorder distribution\n",
    "        if pipeline_results['timeseries_features'] is not None:\n",
    "            ts_df = pipeline_results['timeseries_features']\n",
    "            if 'disorder_label' in ts_df.columns:\n",
    "                disorder_dist = ts_df['disorder_label'].value_counts()\n",
    "                print(\"Disorder Distribution Analysis:\")\n",
    "                for disorder, count in disorder_dist.items():\n",
    "                    percentage = (count / len(ts_df)) * 100\n",
    "                    print(f\"  - {disorder}: {count} samples ({percentage:.1f}%)\")\n",
    "                \n",
    "                pipeline_results['processing_metrics']['disorder_distribution'] = disorder_dist.to_dict()\n",
    "        \n",
    "        # Analyze tracking system integrity\n",
    "        if pipeline_results['timeseries_features'] is not None:\n",
    "            ts_df = pipeline_results['timeseries_features']\n",
    "            tracking_cols = ['global_timeline', 'video_sequence_id', 'timeseries_index']\n",
    "            available_tracking = [col for col in tracking_cols if col in ts_df.columns]\n",
    "            \n",
    "            print(\"Tracking System Analysis:\")\n",
    "            for col in available_tracking:\n",
    "                unique_vals = ts_df[col].nunique()\n",
    "                print(f\"  - {col}: {unique_vals} unique values\")\n",
    "            \n",
    "            # Check for timeline continuity\n",
    "            if 'global_timeline' in ts_df.columns:\n",
    "                timeline_gaps = ts_df['global_timeline'].diff()\n",
    "                large_gaps = timeline_gaps[timeline_gaps > 10].count()  # Gaps > 10 seconds\n",
    "                print(f\"  - Timeline gaps > 10s: {large_gaps}\")\n",
    "        \n",
    "        # Calculate processing efficiency\n",
    "        if pipeline_results['processing_metrics'].get('total_videos_found', 0) > 0:\n",
    "            successful_videos = 0\n",
    "            if pipeline_results['aggregated_features'] is not None:\n",
    "                successful_videos = len(pipeline_results['aggregated_features'])\n",
    "            \n",
    "            efficiency = (successful_videos / pipeline_results['processing_metrics']['total_videos_found']) * 100\n",
    "            pipeline_results['processing_metrics']['processing_efficiency'] = efficiency\n",
    "            print(f\"Processing Efficiency: {efficiency:.1f}% ({successful_videos}/{pipeline_results['processing_metrics']['total_videos_found']} videos)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"ERROR in data analysis: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        pipeline_results['error_log'].append(error_msg)\n",
    "    \n",
    "    # Step 5: Final Pipeline Summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"         PIPELINE EXECUTION SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Success metrics\n",
    "    aggregated_count = len(pipeline_results['aggregated_features']) if pipeline_results['aggregated_features'] is not None else 0\n",
    "    timeseries_count = len(pipeline_results['timeseries_features']) if pipeline_results['timeseries_features'] is not None else 0\n",
    "    \n",
    "    print(f\"Aggregated Features: {aggregated_count} samples\")\n",
    "    print(f\"Timeseries Features: {timeseries_count} samples\")\n",
    "    print(f\"Validation Reports: {len(pipeline_results['validation_reports'])}\")\n",
    "    print(f\"Errors Encountered: {len(pipeline_results['error_log'])}\")\n",
    "    \n",
    "    # Status determination\n",
    "    if aggregated_count > 0 and timeseries_count > 0:\n",
    "        status = \"SUCCESS\"\n",
    "        print(f\"\\nPIPELINE STATUS: {status}\")\n",
    "        print(\"All extraction modes completed successfully!\")\n",
    "    elif aggregated_count > 0 or timeseries_count > 0:\n",
    "        status = \"PARTIAL SUCCESS\"\n",
    "        print(f\"\\nPIPELINE STATUS: {status}\")\n",
    "        print(\"Some extraction modes completed successfully.\")\n",
    "    else:\n",
    "        status = \"FAILED\"\n",
    "        print(f\"\\nPIPELINE STATUS: {status}\")\n",
    "        print(\"No features were successfully extracted.\")\n",
    "    \n",
    "    pipeline_results['processing_metrics']['pipeline_status'] = status\n",
    "    \n",
    "    # Error summary\n",
    "    if pipeline_results['error_log']:\n",
    "        print(\"\\nErrors encountered during processing:\")\n",
    "        for i, error in enumerate(pipeline_results['error_log'], 1):\n",
    "            print(f\"  {i}. {error}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Pipeline execution completed.\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return pipeline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ac9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN EXECUTION: Run Complete Gait Analysis Pipeline\n",
    "# This single function call executes all gait analysis steps in sequential order\n",
    "\n",
    "print(\"STARTING COMPLETE GAIT ANALYSIS PIPELINE...\")\n",
    "print(\"This will execute all processing steps in the correct sequence:\")\n",
    "print(\"1. Video Discovery  2. Aggregated Features  3. Timeseries Features  4. Validation  5. Reporting\")\n",
    "print()\n",
    "\n",
    "# Execute the complete pipeline\n",
    "pipeline_results = run_complete_gait_analysis_pipeline(\n",
    "    dataset_path=VIDEO_DATASET_PATH,\n",
    "    output_dir=\"../Datasets\",\n",
    "    max_cycles_per_video=5,\n",
    "    timeseries_sample_rate_hz=10,\n",
    "    supported_extensions=['.mp4', '.avi', '.mov', '.mkv'],\n",
    "    validation_enabled=True\n",
    ")\n",
    "\n",
    "# Access results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PIPELINE RESULTS SUMMARY:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if pipeline_results['aggregated_features'] is not None:\n",
    "    print(f\" Aggregated dataset: {len(pipeline_results['aggregated_features'])} samples\")\n",
    "    print(f\"  Columns: {len(pipeline_results['aggregated_features'].columns)}\")\n",
    "\n",
    "if pipeline_results['timeseries_features'] is not None:\n",
    "    print(f\" Timeseries dataset: {len(pipeline_results['timeseries_features'])} samples\")\n",
    "    print(f\"  Columns: {len(pipeline_results['timeseries_features'].columns)}\")\n",
    "\n",
    "print(f\" Processing status: {pipeline_results['processing_metrics'].get('pipeline_status', 'Unknown')}\")\n",
    "print(f\" Validation reports: {len(pipeline_results['validation_reports'])}\")\n",
    "\n",
    "if pipeline_results['error_log']:\n",
    "    print(f\" Errors logged: {len(pipeline_results['error_log'])}\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1041d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALTERNATIVE: Quick Start - Single Line Execution\n",
    "# For users who want the simplest possible execution\n",
    "\n",
    "def quick_gait_analysis(dataset_path, output_name=\"gait_analysis_results\"):\n",
    "    \"\"\"\n",
    "    Simplified one-line gait analysis execution.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path (str): Path to video dataset\n",
    "        output_name (str): Base name for output files\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (aggregated_df, timeseries_df) - The two main datasets\n",
    "    \"\"\"\n",
    "    print(f\" QUICK GAIT ANALYSIS: {dataset_path}\")\n",
    "    print(\"Running complete pipeline with default settings...\")\n",
    "    \n",
    "    # Run full pipeline with sensible defaults\n",
    "    results = run_complete_gait_analysis_pipeline(\n",
    "        dataset_path=dataset_path,\n",
    "        max_cycles_per_video=5,\n",
    "        timeseries_sample_rate_hz=10,\n",
    "        validation_enabled=True\n",
    "    )\n",
    "    \n",
    "    # Return the two main datasets\n",
    "    aggregated_data = results['aggregated_features']\n",
    "    timeseries_data = results['timeseries_features']\n",
    "    \n",
    "    print(\" QUICK ANALYSIS COMPLETE!\")\n",
    "    print(f\"   - Aggregated: {len(aggregated_data) if aggregated_data is not None else 0} samples\")\n",
    "    print(f\"   - Timeseries: {len(timeseries_data) if timeseries_data is not None else 0} samples\")\n",
    "    \n",
    "    return aggregated_data, timeseries_data\n",
    "\n",
    "# Example of the simplest possible usage:\n",
    "# aggregated_df, timeseries_df = quick_gait_analysis(VIDEO_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df0081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the improved system with robust data validation\n",
    "print(\"=== TESTING IMPROVED GAIT EXTRACTION SYSTEM ===\")\n",
    "print(\"Extracting with enhanced data validation and NaN prevention...\")\n",
    "\n",
    "# Run extraction with improved data validation\n",
    "clean_timeseries_df = extract_gait_analysis_features(\n",
    "    video_dataset_paths=VIDEO_DATASET_PATH,\n",
    "    output_filename='clean_timeseries_gait_analysis.csv',\n",
    "    max_cycles_per_video=5,\n",
    "    extraction_mode='timeseries',\n",
    "    timeseries_sample_rate_hz=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2a22230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Enhanced Flexible Navigation ===\n",
      "\n",
      "--- Test 1: Enhanced function with aggregated features ---\n",
      "Processing video 1/11: 001_KOA_01_EL.MOV\n",
      "Error processing C:\\Users\\Josemiles\\Desktop\\Datasets\\After\\KOA-PD-NM\\Knee Osteoarthritis\\Knee Osteoarthritis Early Level\\001_KOA_01_EL.MOV: name 'initialize_video_processing' is not defined\n",
      "Processing video 2/11: 001_KOA_02_EL.MOV\n",
      "Error processing C:\\Users\\Josemiles\\Desktop\\Datasets\\After\\KOA-PD-NM\\Knee Osteoarthritis\\Knee Osteoarthritis Early Level\\001_KOA_02_EL.MOV: name 'initialize_video_processing' is not defined\n",
      "Processing video 3/11: 002_KOA_01_EL.MOV\n",
      "Error processing C:\\Users\\Josemiles\\Desktop\\Datasets\\After\\KOA-PD-NM\\Knee Osteoarthritis\\Knee Osteoarthritis Early Level\\002_KOA_01_EL.MOV: name 'initialize_video_processing' is not defined\n",
      "Processing video 4/11: 006_KOA_01_MD.MOV\n",
      "Error processing C:\\Users\\Josemiles\\Desktop\\Datasets\\After\\KOA-PD-NM\\Knee Osteoarthritis\\Knee Osteoarthritis Moderate Disease\\006_KOA_01_MD.MOV: name 'initialize_video_processing' is not defined\n",
      "Processing video 5/11: 014_KOA_02_MD.MOV\n",
      "Error processing C:\\Users\\Josemiles\\Desktop\\Datasets\\After\\KOA-PD-NM\\Knee Osteoarthritis\\Knee Osteoarthritis Moderate Disease\\014_KOA_02_MD.MOV: name 'initialize_video_processing' is not defined\n",
      "Processing video 6/11: 001_KOA_01_SV.MOV\n",
      "Error processing C:\\Users\\Josemiles\\Desktop\\Datasets\\After\\KOA-PD-NM\\Knee Osteoarthritis\\Knee Osteoarthritis Severe\\001_KOA_01_SV.MOV: name 'initialize_video_processing' is not defined\n",
      "Processing video 7/11: 012_KOA_02_SV.MOV\n",
      "Error processing C:\\Users\\Josemiles\\Desktop\\Datasets\\After\\KOA-PD-NM\\Knee Osteoarthritis\\Knee Osteoarthritis Severe\\012_KOA_02_SV.MOV: name 'initialize_video_processing' is not defined\n",
      "Processing video 8/11: 030_NM_01.MOV\n",
      "Error processing C:\\Users\\Josemiles\\Desktop\\Datasets\\After\\KOA-PD-NM\\Normal\\030_NM_01.MOV: name 'initialize_video_processing' is not defined\n",
      "Processing video 9/11: 001_PD_01_SV.MOV\n",
      "Error processing C:\\Users\\Josemiles\\Desktop\\Datasets\\After\\KOA-PD-NM\\Parkinsons Disease\\Parkinsons Disease  Severe stage\\001_PD_01_SV.MOV: name 'initialize_video_processing' is not defined\n",
      "Processing video 10/11: 001_PD_01_ML.MOV\n",
      "Error processing C:\\Users\\Josemiles\\Desktop\\Datasets\\After\\KOA-PD-NM\\Parkinsons Disease\\Parkinsons Disease Mild stage\\001_PD_01_ML.MOV: name 'initialize_video_processing' is not defined\n",
      "Processing video 11/11: 001_PD_01_MD.MOV\n",
      "Error processing C:\\Users\\Josemiles\\Desktop\\Datasets\\After\\KOA-PD-NM\\Parkinsons Disease\\Parkinsons Disease Moderate stage\\001_PD_01_MD.MOV: name 'initialize_video_processing' is not defined\n",
      "Saved 0 rows to Datasets\\enhanced_flexible_navigation_test.csv\n",
      "Data type: Video-level aggregated\n",
      "Enhanced extraction result: 0 video summaries\n",
      "\n",
      "--- Available CSV files in Datasets ---\n",
      "  enhanced_flexible_navigation_test.csv: 0.0 KB\n",
      "  Final_Gait_Features_Named.csv: 5338.7 KB\n",
      "  test_empty.csv: 0.0 KB\n",
      "  test_empty_restored.csv: 0.0 KB\n",
      "  test_enhanced_empty.csv: 0.0 KB\n"
     ]
    }
   ],
   "source": [
    "# Test the enhanced flexible navigation\n",
    "print(\"=== Testing Enhanced Flexible Navigation ===\")\n",
    "\n",
    "# Test 1: Use the enhanced function with flexible navigation\n",
    "print(\"\\n--- Test 1: Enhanced function with aggregated features ---\")\n",
    "df_enhanced = extract_enhanced_gait_features_to_csv(\n",
    "    video_paths=video_paths,\n",
    "    output_csv='enhanced_flexible_navigation_test.csv',\n",
    "    include_raw_frames=False,  # Aggregated features\n",
    "    cycles_per_video=3  # Fewer cycles for faster testing\n",
    ")\n",
    "\n",
    "print(f\"Enhanced extraction result: {len(df_enhanced)} video summaries\")\n",
    "if len(df_enhanced) > 0:\n",
    "    print(\"Columns:\", list(df_enhanced.columns))\n",
    "    if 'disorder_label' in df_enhanced.columns:\n",
    "        print(\"Disorder labels found:\", df_enhanced['disorder_label'].value_counts().to_dict())\n",
    "    if 'total_frames' in df_enhanced.columns:\n",
    "        print(\"Total frames per video:\", df_enhanced[['video_id', 'total_frames', 'extracted_frames']].head())\n",
    "\n",
    "print(\"\\n--- Available CSV files in Datasets ---\")\n",
    "datasets_dir = Path('Datasets')\n",
    "if datasets_dir.exists():\n",
    "    csv_files = list(datasets_dir.glob('*.csv'))\n",
    "    for csv_file in csv_files:\n",
    "        size_kb = csv_file.stat().st_size / 1024\n",
    "        print(f\"  {csv_file.name}: {size_kb:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62b0d3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Navigation Results Summary ===\n",
      "Latest CSV: enhanced_flexible_navigation_test.csv\n",
      "Error reading CSV: No columns to parse from file\n"
     ]
    }
   ],
   "source": [
    "# Quick validation of navigation results\n",
    "print(\"=== Navigation Results Summary ===\")\n",
    "\n",
    "# Check if any videos were found and processed\n",
    "datasets_path = Path('Datasets')\n",
    "if datasets_path.exists():\n",
    "    csv_files = list(datasets_path.glob('*.csv'))\n",
    "    latest_csv = max(csv_files, key=lambda f: f.stat().st_mtime) if csv_files else None\n",
    "    \n",
    "    if latest_csv:\n",
    "        print(f\"Latest CSV: {latest_csv.name}\")\n",
    "        \n",
    "        # Read and show key info about the latest results\n",
    "        try:\n",
    "            df_check = pd.read_csv(latest_csv)\n",
    "            print(f\"Rows processed: {len(df_check)}\")\n",
    "            \n",
    "            if 'disorder_label' in df_check.columns:\n",
    "                disorder_counts = df_check['disorder_label'].value_counts()\n",
    "                print(f\"Gait patterns found: {disorder_counts.to_dict()}\")\n",
    "            \n",
    "            if 'total_frames' in df_check.columns and 'extracted_frames' in df_check.columns:\n",
    "                print(\"Frame extraction summary:\")\n",
    "                for _, row in df_check[['video_id', 'total_frames', 'extracted_frames', 'extraction_efficiency']].head(3).iterrows():\n",
    "                    print(f\"  {row['video_id']}: {row['extracted_frames']}/{row['total_frames']} frames ({row['extraction_efficiency']:.2%})\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading CSV: {e}\")\n",
    "    else:\n",
    "        print(\"No CSV files found\")\n",
    "else:\n",
    "    print(\"Datasets directory not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e39f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_temporal_features(df_video):\n",
    "    \"\"\"\n",
    "    Compute temporal gait features for a single video (all frames belong to same disorder).\n",
    "    This creates summary statistics that capture temporal dynamics while maintaining \n",
    "    video-level coherence for musculoskeletal disorder classification.\n",
    "    \"\"\"\n",
    "    if len(df_video) == 0:\n",
    "        return {}\n",
    "        \n",
    "    temporal_features = {}\n",
    "    \n",
    "    # Video-level temporal info\n",
    "    temporal_features['extracted_frames'] = len(df_video)  # Frames with detected gait cycles\n",
    "    temporal_features['video_duration'] = df_video['video_duration'].iloc[0] if 'video_duration' in df_video else 0\n",
    "    temporal_features['avg_fps'] = df_video['fps'].iloc[0] if 'fps' in df_video else 30\n",
    "    \n",
    "    # Get actual total frames from video metadata\n",
    "    if 'total_frames' in df_video and len(df_video) > 0:\n",
    "        temporal_features['total_frames'] = int(df_video['total_frames'].iloc[0])\n",
    "    else:\n",
    "        # Fallback: calculate from duration and fps\n",
    "        if temporal_features['video_duration'] > 0 and temporal_features['avg_fps'] > 0:\n",
    "            temporal_features['total_frames'] = int(temporal_features['video_duration'] * temporal_features['avg_fps'])\n",
    "        else:\n",
    "            temporal_features['total_frames'] = temporal_features['extracted_frames']\n",
    "    \n",
    "    # Calculate extraction efficiency\n",
    "    if temporal_features['total_frames'] > 0:\n",
    "        temporal_features['extraction_efficiency'] = temporal_features['extracted_frames'] / temporal_features['total_frames']\n",
    "    else:\n",
    "        temporal_features['extraction_efficiency'] = 0.0\n",
    "    \n",
    "    # Gait cycle statistics\n",
    "    if 'step_time_sec' in df_video:\n",
    "        step_times = df_video['step_time_sec'].dropna()\n",
    "        if len(step_times) > 0:\n",
    "            temporal_features['avg_step_time'] = step_times.mean()\n",
    "            temporal_features['std_step_time'] = step_times.std()\n",
    "            temporal_features['cadence_steps_per_min'] = 60.0 / step_times.mean() if step_times.mean() > 0 else 0\n",
    "    \n",
    "    # Joint angle temporal dynamics (capture movement patterns)\n",
    "    angle_cols = [col for col in df_video.columns if 'angle' in col]\n",
    "    for angle_col in angle_cols:\n",
    "        if angle_col in df_video:\n",
    "            angles = df_video[angle_col].dropna()\n",
    "            if len(angles) > 1:\n",
    "                temporal_features[f'{angle_col}_mean'] = angles.mean()\n",
    "                temporal_features[f'{angle_col}_std'] = angles.std()\n",
    "                temporal_features[f'{angle_col}_range'] = angles.max() - angles.min()\n",
    "                # Temporal derivative (rate of change)\n",
    "                angle_diff = angles.diff().dropna()\n",
    "                if len(angle_diff) > 0:\n",
    "                    temporal_features[f'{angle_col}_velocity_mean'] = angle_diff.mean()\n",
    "                    temporal_features[f'{angle_col}_velocity_std'] = angle_diff.std()\n",
    "    \n",
    "    # Stride characteristics\n",
    "    if 'stride_length' in df_video:\n",
    "        strides = df_video['stride_length'].dropna()\n",
    "        if len(strides) > 0:\n",
    "            temporal_features['avg_stride_length'] = strides.mean()\n",
    "            temporal_features['std_stride_length'] = strides.std()\n",
    "            temporal_features['stride_consistency'] = 1.0 - (strides.std() / strides.mean()) if strides.mean() > 0 else 0\n",
    "    \n",
    "    # Temporal symmetry (left vs right limb timing)\n",
    "    left_heel_y = df_video.get('LEFT_HEEL_y', pd.Series())\n",
    "    right_heel_y = df_video.get('RIGHT_HEEL_y', pd.Series())\n",
    "    if len(left_heel_y) > 1 and len(right_heel_y) > 1:\n",
    "        left_movement = left_heel_y.diff().dropna()\n",
    "        right_movement = right_heel_y.diff().dropna()\n",
    "        if len(left_movement) > 0 and len(right_movement) > 0:\n",
    "            # Cross-correlation for temporal symmetry\n",
    "            min_len = min(len(left_movement), len(right_movement))\n",
    "            left_move_norm = (left_movement[:min_len] - left_movement[:min_len].mean()) / left_movement[:min_len].std()\n",
    "            right_move_norm = (right_movement[:min_len] - right_movement[:min_len].mean()) / right_movement[:min_len].std()\n",
    "            temporal_features['limb_symmetry'] = np.corrcoef(left_move_norm, right_move_norm)[0, 1] if min_len > 1 else 0\n",
    "    \n",
    "    return temporal_features\n",
    "\n",
    "print('temporal feature computation ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953237a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enhanced gait feature extraction ready\n"
     ]
    }
   ],
   "source": [
    "def extract_enhanced_gait_features_to_csv(video_paths, output_csv='enhanced_gait_features.csv', \n",
    "                                       cycles_per_video=5, allowed_exts=None, include_raw_frames=False):\n",
    "    \"\"\"\n",
    "    Enhanced gait feature extraction with temporal coherence for musculoskeletal disorder analysis.\n",
    "\n",
    "    Args:\n",
    "        video_paths: List of video file/folder paths\n",
    "        output_csv: Output CSV filename\n",
    "        cycles_per_video: Number of gait cycles to extract per video\n",
    "        allowed_exts: Video file extensions to process\n",
    "        include_raw_frames: If True, save frame-by-frame data; if False, save aggregated features per video\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with extracted features\n",
    "    \"\"\"\n",
    "    allowed_exts = allowed_exts or ['.mp4', '.avi', '.mov', '.mkv']\n",
    "\n",
    "    if isinstance(video_paths, (str, Path)):\n",
    "        video_paths = [video_paths]\n",
    "\n",
    "    all_rows = []\n",
    "    video_items = []\n",
    "\n",
    "    # Collect video files with flexible navigation\n",
    "    def find_videos_recursively(path, max_depth=10, current_depth=0):\n",
    "        \"\"\"Recursively find video files, returning videos found in the first directory that contains them.\"\"\"\n",
    "        videos_found = []\n",
    "        path = Path(path)\n",
    "\n",
    "        if not path.exists() or current_depth > max_depth:\n",
    "            return videos_found\n",
    "\n",
    "        if path.is_file() and path.suffix.lower() in allowed_exts:\n",
    "            return [path]\n",
    "\n",
    "        if path.is_dir():\n",
    "            direct_videos = [child for child in sorted(path.iterdir()) if child.is_file() and child.suffix.lower() in allowed_exts]\n",
    "            if direct_videos:\n",
    "                return direct_videos\n",
    "\n",
    "            for subdir in sorted([c for c in path.iterdir() if c.is_dir()]):\n",
    "                vids = find_videos_recursively(subdir, max_depth, current_depth + 1)\n",
    "                if vids:\n",
    "                    return vids\n",
    "\n",
    "        return videos_found\n",
    "\n",
    "    for p in video_paths:\n",
    "        video_items.extend(find_videos_recursively(p))\n",
    "\n",
    "    if not video_items:\n",
    "        print('No video files found in provided paths. Writing empty CSV.')\n",
    "        out_dir = Path('Datasets')\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        df = pd.DataFrame()\n",
    "        out_path = out_dir / output_csv\n",
    "        df.to_csv(out_path, index=False)\n",
    "        print(f'Saved empty CSV to {out_path}')\n",
    "        return df\n",
    "\n",
    "    # Group videos by the folder where they were found and process\n",
    "    videos_by_folder = {}\n",
    "    for v in video_items:\n",
    "        parent = Path(v).parent\n",
    "        videos_by_folder.setdefault(parent, []).append(v)\n",
    "\n",
    "    for video_idx, (folder, vids) in enumerate(videos_by_folder.items()):\n",
    "        folder_name = folder.name or 'unknown'\n",
    "        print(f'Processing folder {video_idx+1}/{len(videos_by_folder)}: {folder} (label: {folder_name})')\n",
    "        for video_path in vids:\n",
    "            print(f'  - {video_path.name}')\n",
    "            try:\n",
    "                frame_rows = process_video_for_gait(str(video_path), cycles_per_video=cycles_per_video)\n",
    "                if not frame_rows:\n",
    "                    print(f'    No gait cycles detected in {video_path.name}')\n",
    "                    continue\n",
    "\n",
    "                # Add video-level metadata to each frame\n",
    "                for row in frame_rows:\n",
    "                    row['video_id'] = video_path.stem\n",
    "                    row['video_path'] = str(video_path)\n",
    "                    row['disorder_label'] = folder_name\n",
    "                    row['gait_pattern'] = folder_name  # Backwards compatibility\n",
    "\n",
    "                if include_raw_frames:\n",
    "                    all_rows.extend(frame_rows)\n",
    "                else:\n",
    "                    df_video = pd.DataFrame(frame_rows)\n",
    "                    temporal_features = compute_temporal_features(df_video)\n",
    "\n",
    "                    video_summary = {\n",
    "                        'video_id': video_path.stem,\n",
    "                        'video_path': str(video_path),\n",
    "                        'disorder_label': folder_name,\n",
    "                        'gait_pattern': folder_name,\n",
    "                        'total_gait_cycles': len(df_video) // max(1, len(df_video['cycle_start_time'].dropna().unique())),\n",
    "                        **temporal_features\n",
    "                    }\n",
    "                    all_rows.append(video_summary)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'Error processing {video_path}: {e}')\n",
    "                continue\n",
    "\n",
    "    # Save results\n",
    "    out_dir = Path('Datasets')\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    out_path = out_dir / output_csv\n",
    "    df.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f'Saved {len(df)} rows to {out_path}')\n",
    "    print(f'Data type: {\"Frame-by-frame\" if include_raw_frames else \"Video-level aggregated\"}')\n",
    "    if 'disorder_label' in df.columns:\n",
    "        print(f'Disorder distribution: {df[\"disorder_label\"].value_counts().to_dict()}')\n",
    "\n",
    "    return df\n",
    "\n",
    "print('enhanced gait feature extraction ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Extract aggregated temporal features (one row per video)\n",
    "# This is ideal for traditional ML classifiers - each video becomes one sample with its disorder label\n",
    "print(\"=== Example 1: Video-level aggregated features ===\")\n",
    "# df_aggregated = extract_enhanced_gait_features_to_csv(\n",
    "#     video_paths=['path/to/video_folder/'],  # Folder with labeled videos\n",
    "#     output_csv='aggregated_gait_features.csv',\n",
    "#     include_raw_frames=False  # One row per video with temporal summary features\n",
    "# )\n",
    "\n",
    "print(\"=== Example 2: Frame-by-frame temporal sequences ===\")\n",
    "# This preserves full temporal information for sequence models (RNN/LSTM)\n",
    "# df_sequences = extract_enhanced_gait_features_to_csv(\n",
    "#     video_paths=['path/to/video_folder/'],\n",
    "#     output_csv='temporal_gait_sequences.csv', \n",
    "#     include_raw_frames=True  # Multiple rows per video, preserving time order\n",
    "# )\n",
    "\n",
    "print(\"=== Example 3: Test with existing empty folder ===\")\n",
    "# Test on empty list (should create empty CSV)\n",
    "df_test = extract_enhanced_gait_features_to_csv(\n",
    "    video_paths=[],\n",
    "    output_csv='test_enhanced_empty.csv',\n",
    "    include_raw_frames=False\n",
    ")\n",
    "print(f\"Test result: {len(df_test)} rows\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GaitEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
