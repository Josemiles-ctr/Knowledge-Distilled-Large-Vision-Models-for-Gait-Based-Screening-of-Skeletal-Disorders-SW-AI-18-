{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"82a43efc3f05455292fb9e8c7a8c7aac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_999add2f892347c6b96b153b2f98b148","IPY_MODEL_622d44f3c82b471dbc740485e79a23ef","IPY_MODEL_fee30bd7fe1c45a0a0189b99e3ed3208"],"layout":"IPY_MODEL_6eb1d51d2d3d4f9ba042e057f81f041f"}},"999add2f892347c6b96b153b2f98b148":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e27f4085ca014faf8958647c7c8897e8","placeholder":"​","style":"IPY_MODEL_47915d8a47db4a59bfa00872fba633a4","value":"model.safetensors: 100%"}},"622d44f3c82b471dbc740485e79a23ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_684456b2ec23490c802bad2a21a2475a","max":205948668,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ff10755833b34fc9943f320e787392f4","value":205948668}},"fee30bd7fe1c45a0a0189b99e3ed3208":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7cb53a4d9a144c5ade01bebc67fb643","placeholder":"​","style":"IPY_MODEL_f71a8330f336490c9d54cfc0d429297e","value":" 206M/206M [00:12&lt;00:00, 19.7MB/s]"}},"6eb1d51d2d3d4f9ba042e057f81f041f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e27f4085ca014faf8958647c7c8897e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47915d8a47db4a59bfa00872fba633a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"684456b2ec23490c802bad2a21a2475a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff10755833b34fc9943f320e787392f4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b7cb53a4d9a144c5ade01bebc67fb643":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f71a8330f336490c9d54cfc0d429297e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ce39e8b1c3e4cc2981c217900a1ceda":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_476a5e23aa8845d4b8a9d399f4992009","IPY_MODEL_5278a447c22c47a7aeeac25dce5d3d0f","IPY_MODEL_1db8c7201585414b8f11caf4f9911daa"],"layout":"IPY_MODEL_62df8a559173415eba014d652763faba"}},"476a5e23aa8845d4b8a9d399f4992009":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a84b321e06984c93a0e73f1e3d2f191d","placeholder":"​","style":"IPY_MODEL_316051b72c9a41e08db457027daf7afd","value":"config.json: 100%"}},"5278a447c22c47a7aeeac25dce5d3d0f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b42a00b1aa14f96a16c8cc9c5c99658","max":385,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6fa2e848d974ebe93aab56de7fd3a80","value":385}},"1db8c7201585414b8f11caf4f9911daa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bcde6503d38d4e73b4aadbd52e6982b7","placeholder":"​","style":"IPY_MODEL_2b6b7d5809ab4d208c71caf83a7c2bdb","value":" 385/385 [00:00&lt;00:00, 44.9kB/s]"}},"62df8a559173415eba014d652763faba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a84b321e06984c93a0e73f1e3d2f191d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"316051b72c9a41e08db457027daf7afd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4b42a00b1aa14f96a16c8cc9c5c99658":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6fa2e848d974ebe93aab56de7fd3a80":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bcde6503d38d4e73b4aadbd52e6982b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b6b7d5809ab4d208c71caf83a7c2bdb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72f43a60afef42648a8cc1b96df041bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5cabe617ee1f4903bfa7b5fbd9ad6cdb","IPY_MODEL_8d4fa5d5a1ac4c5c806567f984e37a8f","IPY_MODEL_82d00d71fcc3439c91b079510c130c35"],"layout":"IPY_MODEL_196a54c92fd8465fa2587965e071b529"}},"5cabe617ee1f4903bfa7b5fbd9ad6cdb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b3a3aa0f1e64bf99ac6e45a02ed0d83","placeholder":"​","style":"IPY_MODEL_17dbfa5bc70a4865afd45228b454b49d","value":"vocab.txt: "}},"8d4fa5d5a1ac4c5c806567f984e37a8f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ccf80f49c4844e43a9a5df1efffbc2b3","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a303c57233cf4edbb682ccf1fff84c11","value":1}},"82d00d71fcc3439c91b079510c130c35":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd8089195dd44ede930b95ccb17f26e6","placeholder":"​","style":"IPY_MODEL_be56cf59ca0a4c82a140b12eddf0d7f7","value":" 213k/? [00:00&lt;00:00, 12.3MB/s]"}},"196a54c92fd8465fa2587965e071b529":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b3a3aa0f1e64bf99ac6e45a02ed0d83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17dbfa5bc70a4865afd45228b454b49d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ccf80f49c4844e43a9a5df1efffbc2b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"a303c57233cf4edbb682ccf1fff84c11":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bd8089195dd44ede930b95ccb17f26e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be56cf59ca0a4c82a140b12eddf0d7f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1c6917cff494266aa95f54e2bbc9771":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dce7ecf85e9d410c929ae05e5e22f9ad","IPY_MODEL_98c0dc1b70a1470688b134b8b51fc8cf","IPY_MODEL_5d8d454e02a540cf970c2d10a64b16f4"],"layout":"IPY_MODEL_5a7ba236638944e595d7d034fd785ae0"}},"dce7ecf85e9d410c929ae05e5e22f9ad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bdc4ba8871d54b10adafd524a5a3c81d","placeholder":"​","style":"IPY_MODEL_367ace1e1dfa4e6a9edfb8ffba99caae","value":"pytorch_model.bin: 100%"}},"98c0dc1b70a1470688b134b8b51fc8cf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e492cf63ae3c4c0a98846eabfbae2ff9","max":435778770,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eb5e359c39724be9893b3f9bd6a5cae8","value":435778770}},"5d8d454e02a540cf970c2d10a64b16f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42dcc02b842d4e3cbefd0c39a9efbbde","placeholder":"​","style":"IPY_MODEL_9d41c1e1510d4e58accaa9e6e65b8d13","value":" 436M/436M [00:07&lt;00:00, 69.4MB/s]"}},"5a7ba236638944e595d7d034fd785ae0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdc4ba8871d54b10adafd524a5a3c81d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"367ace1e1dfa4e6a9edfb8ffba99caae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e492cf63ae3c4c0a98846eabfbae2ff9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb5e359c39724be9893b3f9bd6a5cae8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"42dcc02b842d4e3cbefd0c39a9efbbde":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d41c1e1510d4e58accaa9e6e65b8d13":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13507864,"sourceType":"datasetVersion","datasetId":8576380}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade --no-deps pytorchvideo==0.1.5 timm==1.0.19 decord==0.6.0 opencv-python==4.12.0.88 transformers==4.53.3 sentencepiece==0.2.0 sentence_transformers==2.2.2\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EzSX0iTqQ7so","outputId":"742b7dd4-a0a4-4d12-e1e4-c80cf878979e","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:28:51.888043Z","iopub.execute_input":"2025-10-28T10:28:51.888708Z","iopub.status.idle":"2025-10-28T10:28:58.137940Z","shell.execute_reply.started":"2025-10-28T10:28:51.888681Z","shell.execute_reply":"2025-10-28T10:28:58.136954Z"}},"outputs":[{"name":"stdout","text":"Collecting pytorchvideo==0.1.5\n  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: timm==1.0.19 in /usr/local/lib/python3.11/dist-packages (1.0.19)\nRequirement already satisfied: decord==0.6.0 in /usr/local/lib/python3.11/dist-packages (0.6.0)\nRequirement already satisfied: opencv-python==4.12.0.88 in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\nRequirement already satisfied: transformers==4.53.3 in /usr/local/lib/python3.11/dist-packages (4.53.3)\nRequirement already satisfied: sentencepiece==0.2.0 in /usr/local/lib/python3.11/dist-packages (0.2.0)\nCollecting sentence_transformers==2.2.2\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: pytorchvideo, sentence_transformers\n  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188686 sha256=d9341cb3991308cc4bc20c40919d5cb1147420d61070402b99f6967dbb7f6920\n  Stored in directory: /root/.cache/pip/wheels/a4/6d/ae/d016375a73be141a0e11bb42289e2d0b046c35687fc8010ecc\n  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=2cf5c5ac62aa37fee93a38d60bf17bd75912a0a05ad1c3d0d629575902028aa1\n  Stored in directory: /root/.cache/pip/wheels/ff/27/bf/ffba8b318b02d7f691a57084ee154e26ed24d012b0c7805881\nSuccessfully built pytorchvideo sentence_transformers\nInstalling collected packages: sentence_transformers, pytorchvideo\n  Attempting uninstall: sentence_transformers\n    Found existing installation: sentence-transformers 4.1.0\n    Uninstalling sentence-transformers-4.1.0:\n      Successfully uninstalled sentence-transformers-4.1.0\nSuccessfully installed pytorchvideo-0.1.5 sentence_transformers-2.2.2\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"!pip uninstall -y scikit-learn sklearn\n!pip install --no-cache-dir scikit-learn==1.5.2 numpy==1.26.4 scipy==1.13.1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:29:01.696987Z","iopub.execute_input":"2025-10-28T10:29:01.697281Z","iopub.status.idle":"2025-10-28T10:29:15.046035Z","shell.execute_reply.started":"2025-10-28T10:29:01.697259Z","shell.execute_reply":"2025-10-28T10:29:15.045080Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: scikit-learn 1.2.2\nUninstalling scikit-learn-1.2.2:\n  Successfully uninstalled scikit-learn-1.2.2\n\u001b[33mWARNING: Skipping sklearn as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mCollecting scikit-learn==1.5.2\n  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\nCollecting scipy==1.13.1\n  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.5.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy==1.26.4) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy==1.26.4) (2024.2.0)\nDownloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m276.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m265.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scipy, scikit-learn\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.15.3\n    Uninstalling scipy-1.15.3:\n      Successfully uninstalled scipy-1.15.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scikit-learn-1.5.2 scipy-1.13.1\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir numpy==1.26.4 scipy==1.13.1 scikit-learn==1.5.2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:29:15.047729Z","iopub.execute_input":"2025-10-28T10:29:15.047995Z","iopub.status.idle":"2025-10-28T10:29:19.114717Z","shell.execute_reply.started":"2025-10-28T10:29:15.047976Z","shell.execute_reply":"2025-10-28T10:29:19.113572Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: scipy==1.13.1 in /usr/local/lib/python3.11/dist-packages (1.13.1)\nRequirement already satisfied: scikit-learn==1.5.2 in /usr/local/lib/python3.11/dist-packages (1.5.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4) (2.4.1)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.5.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (3.6.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy==1.26.4) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy==1.26.4) (2024.2.0)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"\n\n# MViT imports\nfrom transformers import AutoModelForVideoClassification, AutoConfig\n# Clinical embeddings imports\nfrom transformers import AutoTokenizer, AutoModel\nimport json\n\n# Setup device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m3I_NXf1V-CN","outputId":"0db4ddbe-9ba5-434b-aad1-e68317bdf08d","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:29:19.115819Z","iopub.execute_input":"2025-10-28T10:29:19.116244Z","iopub.status.idle":"2025-10-28T10:29:19.121843Z","shell.execute_reply.started":"2025-10-28T10:29:19.116219Z","shell.execute_reply":"2025-10-28T10:29:19.121072Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score\nimport cv2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:29:21.833060Z","iopub.execute_input":"2025-10-28T10:29:21.833778Z","iopub.status.idle":"2025-10-28T10:29:21.837684Z","shell.execute_reply.started":"2025-10-28T10:29:21.833750Z","shell.execute_reply":"2025-10-28T10:29:21.836844Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"!pip install decord==0.6.0 --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:29:26.480361Z","iopub.execute_input":"2025-10-28T10:29:26.481109Z","iopub.status.idle":"2025-10-28T10:29:29.535042Z","shell.execute_reply.started":"2025-10-28T10:29:26.481083Z","shell.execute_reply":"2025-10-28T10:29:29.534003Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import decord\nfrom decord import VideoReader, cpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:29:30.626280Z","iopub.execute_input":"2025-10-28T10:29:30.626962Z","iopub.status.idle":"2025-10-28T10:29:30.630773Z","shell.execute_reply.started":"2025-10-28T10:29:30.626937Z","shell.execute_reply":"2025-10-28T10:29:30.630054Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport timm\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:29:35.042559Z","iopub.execute_input":"2025-10-28T10:29:35.043165Z","iopub.status.idle":"2025-10-28T10:29:35.046901Z","shell.execute_reply.started":"2025-10-28T10:29:35.043143Z","shell.execute_reply":"2025-10-28T10:29:35.046211Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import os\nfrom glob import glob\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:29:40.351081Z","iopub.execute_input":"2025-10-28T10:29:40.351825Z","iopub.status.idle":"2025-10-28T10:29:40.355557Z","shell.execute_reply.started":"2025-10-28T10:29:40.351786Z","shell.execute_reply":"2025-10-28T10:29:40.354882Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"class GaitDataset(Dataset):\n    def __init__(self, video_paths, labels, num_frames=16, frame_size=224):\n        self.video_paths = video_paths\n        self.labels = labels\n        self.num_frames = num_frames\n        self.frame_size = frame_size\n\n    def __len__(self):\n        return len(self.video_paths)\n\n    def __getitem__(self, idx):\n        try:\n            vr = VideoReader(self.video_paths[idx], ctx=cpu(0))\n            total_frames = len(vr)\n\n            if total_frames <= self.num_frames:\n                frame_indices = list(range(total_frames))\n                while len(frame_indices) < self.num_frames:\n                    frame_indices.append(frame_indices[-1])\n            else:\n                frame_indices = np.linspace(0, total_frames-1, self.num_frames, dtype=int)\n\n            frames = vr.get_batch(frame_indices).asnumpy()\n\n            # Resize frames\n            resized_frames = []\n            for frame in frames:\n                resized_frame = cv2.resize(frame, (self.frame_size, self.frame_size))\n                resized_frames.append(resized_frame)\n            frames = np.array(resized_frames)\n\n            # Convert to tensor and normalize\n            frames = torch.from_numpy(frames).permute(3, 0, 1, 2).float()\n            if frames.max() > 1.0:\n                frames = frames / 255.0\n\n            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1, 1)\n            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1, 1)\n            frames = (frames - mean) / std\n\n            return frames, self.labels[idx]\n\n        except Exception as e:\n            print(f\"Error loading {self.video_paths[idx]}: {e}\")\n            dummy_frames = torch.randn(3, self.num_frames, self.frame_size, self.frame_size)\n            return dummy_frames, self.labels[idx]","metadata":{"id":"s5G8abZIWBg1","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:29:44.163188Z","iopub.execute_input":"2025-10-28T10:29:44.163906Z","iopub.status.idle":"2025-10-28T10:29:44.171910Z","shell.execute_reply.started":"2025-10-28T10:29:44.163882Z","shell.execute_reply":"2025-10-28T10:29:44.171151Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def load_and_split_dataset(base_path, test_size=0.2, val_size=0.1):\n    # Find all video files and extract labels from folder names\n    video_paths = []\n    labels = []\n    label_to_idx = {}\n    idx_counter = 0\n\n    for root, dirs, files in os.walk(base_path):\n        for file in files:\n            if file.endswith(('.mp4', '.MOV', '.mov')):\n                video_path = os.path.join(root, file)\n                # Use immediate parent folder as label\n                label_name = os.path.basename(root)\n\n                if label_name not in label_to_idx:\n                    label_to_idx[label_name] = idx_counter\n                    idx_counter += 1\n\n                video_paths.append(video_path)\n                labels.append(label_to_idx[label_name])\n\n    # 70:20:10 split (train:test:val)\n    train_val_paths, test_paths, train_val_labels, test_labels = train_test_split(\n        video_paths, labels, test_size=test_size, random_state=42, stratify=labels\n    )\n\n    val_ratio = val_size / (1 - test_size)\n    train_paths, val_paths, train_labels, val_labels = train_test_split(\n        train_val_paths, train_val_labels, test_size=val_ratio, random_state=42, stratify=train_val_labels\n    )\n\n    print(f\"Dataset loaded: {len(video_paths)} videos\")\n    print(f\"Train: {len(train_paths)}, Val: {len(val_paths)}, Test: {len(test_paths)}\")\n    print(f\"Classes: {label_to_idx}\")\n\n    return train_paths, train_labels, val_paths, val_labels, test_paths, test_labels, label_to_idx","metadata":{"id":"4bL9vTYhWHUR","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:29:50.846477Z","iopub.execute_input":"2025-10-28T10:29:50.846838Z","iopub.status.idle":"2025-10-28T10:29:50.853205Z","shell.execute_reply.started":"2025-10-28T10:29:50.846817Z","shell.execute_reply":"2025-10-28T10:29:50.852326Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"from google.colab import drive\n\n# Mount your Google Drive\ndrive.mount('/content/drive', force_remount=True)\n\n# To access shared drives, you might need to authorize it after mounting\n# You can then access shared drives under /content/drive/Shared with me/","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"98df0fcd","outputId":"42fc8163-0a5c-48d5-c5e5-4347077d284c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip freeze requiremets.txt\nprint(\"Requirements file made\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import zipfile\nimport os\n\n# Load your dataset\n\n# base_path = \"/content/drive/MyDrive/GiatLabDatset.zip/\" # Original path, commented out\n\nzip_path = \"/content/drive/MyDrive/GiatLabDatset.zip\"\nunzip_path = \"/content/GaitLabDataset/\"\n\nif os.path.exists(zip_path):\n    print(f\"Unzipping {zip_path} to {unzip_path}\")\n    os.makedirs(unzip_path, exist_ok=True)\n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall(unzip_path)\n    print(\"Unzipping complete.\")\n    # Update base_path to the unzipped directory\n    base_path = unzip_path\nelse:\n    print(f\"Error: Zip file not found at {zip_path}\")\n    # Keep the original base_path if zip file is not found\n    base_path = \"/content/drive/MyDrive/GiatLabDatset.zip/\" # Or handle this case as appropriate for your workflow\n\n\n# Verify the contents of the unzipped directory\nif os.path.exists(base_path):\n    print(f\"Contents of {base_path}:\")\n    try:\n        for item in os.listdir(base_path):\n            print(f\"  - {item}\")\n    except Exception as e:\n        print(f\"Error listing contents: {e}\")\nelse:\n    print(f\"Error: Unzip path not found at {base_path}\")","metadata":{"id":"L2N9rZxLcuu2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"848180f1-ec26-47f8-b6ee-03dfc2e7e101","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/input/gaitlabdataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:30:20.949095Z","iopub.execute_input":"2025-10-28T10:30:20.949721Z","iopub.status.idle":"2025-10-28T10:30:21.102594Z","shell.execute_reply.started":"2025-10-28T10:30:20.949691Z","shell.execute_reply":"2025-10-28T10:30:21.101874Z"}},"outputs":[{"name":"stdout","text":"GiatLabDatset\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# Load your dataset after unzipping\n# Moved the load_and_split_dataset call here\nbase_path=\"/kaggle/input/gaitlabdataset/GiatLabDatset\"\nif os.path.exists(base_path):\n    train_paths, train_labels, val_paths, val_labels, test_paths, test_labels, class_mapping = load_and_split_dataset(base_path)\nelse:\n    print(\"Cannot load dataset as the base path does not exist.\")\n    train_paths, train_labels, val_paths, val_labels, test_paths, test_labels, class_mapping = [], [], [], [], [], [], {}","metadata":{"id":"nhQBPrKUgdeV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9bf63183-cb18-4420-d3d9-a14dc545cfe2","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:31:04.150346Z","iopub.execute_input":"2025-10-28T10:31:04.150959Z","iopub.status.idle":"2025-10-28T10:31:04.519007Z","shell.execute_reply.started":"2025-10-28T10:31:04.150935Z","shell.execute_reply":"2025-10-28T10:31:04.518189Z"}},"outputs":[{"name":"stdout","text":"Dataset loaded: 230 videos\nTrain: 161, Val: 23, Test: 46\nClasses: {'Normal': 0, 'Assistive': 1, 'NonAssistive': 2, 'PD_Mild': 3, 'PD_Early': 4, 'PD_Severe': 5, 'KOA_Early': 6, 'KOA_Mild': 7, 'KOA_Severe': 8}\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# Create datasets\ntrain_dataset = GaitDataset(train_paths, train_labels)\nval_dataset = GaitDataset(val_paths, val_labels)\ntest_dataset = GaitDataset(test_paths, test_labels)\n\n# Create data loaders\nbatch_size = 12\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nprint(f\"Data loaders created with batch size {batch_size}\")","metadata":{"id":"-PPR3RnKXr0c","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b24e6f24-2924-4b7b-8f82-b3245ea0794a","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:31:24.005559Z","iopub.execute_input":"2025-10-28T10:31:24.006298Z","iopub.status.idle":"2025-10-28T10:31:24.012085Z","shell.execute_reply.started":"2025-10-28T10:31:24.006272Z","shell.execute_reply":"2025-10-28T10:31:24.011474Z"}},"outputs":[{"name":"stdout","text":"Data loaders created with batch size 12\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"class MViTTeacher(nn.Module):\n    def __init__(self, num_classes, pretrained=True):\n        super().__init__()\n        # Use a pre-trained MViT from timm and adapt it for video\n        self.backbone = timm.create_model('mvitv2_base', pretrained=pretrained)\n\n        # Modify the head for video classification\n        # Remove the original head\n        self.backbone.head = nn.Identity()\n\n        # Determine the number of features from the backbone's output\n        # We need a dummy forward pass to get the shape\n        # Assuming an input shape like [1, 3, 16, 224, 224] (batch=1, channels=3, frames=16, height=224, width=224)\n        with torch.no_grad():\n            dummy_input = torch.randn(1, 3, 16, 224, 224)\n            # Reshape for the MViT backbone (treating frames as batch)\n            dummy_input = dummy_input.permute(0, 2, 1, 3, 4).contiguous()\n            dummy_input = dummy_input.view(1 * 16, 3, 224, 224)\n            num_ftrs = self.backbone(dummy_input).shape[-1]\n\n\n        # Add a new classification head that expects flattened features [batch_size, num_frames * features_per_frame]\n        self.classifier = nn.Linear(num_ftrs * 16, num_classes) # Assuming 16 frames as defined in GaitDataset\n\n    def forward(self, x):\n        # x is [batch, channels, frames, height, width]\n        batch_size, channels, frames, height, width = x.size()\n\n        # Reshape for the MViT backbone (treating frames as batch)\n        # The timm MViT expects [batch*frames, channels, height, width]\n        x = x.permute(0, 2, 1, 3, 4).contiguous()\n        x = x.view(batch_size * frames, channels, height, width)\n\n        # Pass through the MViT backbone (without the original head)\n        visual_features = self.backbone(x) # Shape: [batch*frames, num_ftrs]\n\n        # Reshape back to separate batch and frames\n        visual_features = visual_features.view(batch_size, frames, -1) # Shape: [batch, frames, num_ftrs]\n\n        # Flatten the temporal and feature dimensions for the new classifier\n        visual_features_flat = visual_features.view(batch_size, -1) # Shape: [batch, frames * num_ftrs]\n\n        # Pass through the new classifier\n        logits = self.classifier(visual_features_flat)\n\n        return logits","metadata":{"id":"ioYTnYDRX2W3","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:31:32.613457Z","iopub.execute_input":"2025-10-28T10:31:32.614216Z","iopub.status.idle":"2025-10-28T10:31:32.620804Z","shell.execute_reply.started":"2025-10-28T10:31:32.614184Z","shell.execute_reply":"2025-10-28T10:31:32.620041Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"num_classes = len(class_mapping)\nteacher_model = MViTTeacher(num_classes=num_classes)\nteacher_model = teacher_model.to(device)\nprint(f\"MViT Teacher initialized with {num_classes} classes\")","metadata":{"id":"8nEshORcX6-Q","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["82a43efc3f05455292fb9e8c7a8c7aac","999add2f892347c6b96b153b2f98b148","622d44f3c82b471dbc740485e79a23ef","fee30bd7fe1c45a0a0189b99e3ed3208","6eb1d51d2d3d4f9ba042e057f81f041f","e27f4085ca014faf8958647c7c8897e8","47915d8a47db4a59bfa00872fba633a4","684456b2ec23490c802bad2a21a2475a","ff10755833b34fc9943f320e787392f4","b7cb53a4d9a144c5ade01bebc67fb643","f71a8330f336490c9d54cfc0d429297e"]},"outputId":"bb1c1622-a4bf-4773-b06e-79bc69b57591","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:31:37.353898Z","iopub.execute_input":"2025-10-28T10:31:37.354470Z","iopub.status.idle":"2025-10-28T10:31:44.854288Z","shell.execute_reply.started":"2025-10-28T10:31:37.354444Z","shell.execute_reply":"2025-10-28T10:31:44.853600Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/206M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52fc1728b1eb45d8bb12d50df0c9aa04"}},"metadata":{}},{"name":"stdout","text":"MViT Teacher initialized with 9 classes\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"class ClinicalEnhancedStudent(nn.Module):\n    def __init__(self, num_classes, clinical_dim=768): # Changed clinical_dim to 768\n        super().__init__()\n\n        # Visual backbone\n        self.visual_encoder = nn.Sequential(\n            nn.Conv3d(3, 16, kernel_size=(3, 3, 3), padding=1),\n            nn.BatchNorm3d(16), nn.ReLU(), nn.MaxPool3d((1, 2, 2)),\n\n            nn.Conv3d(16, 32, kernel_size=(3, 3, 3), padding=1),\n            nn.BatchNorm3d(32), nn.ReLU(), nn.MaxPool3d((1, 2, 2)),\n\n            nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1),\n            nn.BatchNorm3d(64), nn.ReLU(), nn.MaxPool3d((1, 2, 2)),\n\n            nn.AdaptiveAvgPool3d((None, 7, 7))\n        )\n\n        # Clinical embeddings (from language model)\n        self.clinical_proj = nn.Linear(clinical_dim, 128)\n\n        # Fusion classifier\n        # Need to determine the output size of visual_encoder to calculate the input size for the classifier\n        # Let's perform a dummy forward pass\n        with torch.no_grad():\n            dummy_input = torch.randn(1, 3, 16, 224, 224) # Assuming input shape [batch, channels, frames, height, width]\n            visual_output_dummy = self.visual_encoder(dummy_input)\n            visual_flat_size = visual_output_dummy.view(1, -1).shape[-1]\n\n        self.classifier = nn.Sequential(\n            nn.Linear(visual_flat_size + 128, 256), # Updated input size for classifier\n            nn.ReLU(), nn.Dropout(0.3),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x, clinical_embeds=None):\n        # Visual features\n        visual_features = self.visual_encoder(x)\n        batch_size = visual_features.size(0)\n        visual_flat = visual_features.view(batch_size, -1)\n\n        # Clinical features (if provided)\n        if clinical_embeds is not None:\n            clinical_proj = self.clinical_proj(clinical_embeds)\n            fused_features = torch.cat([visual_flat, clinical_proj], dim=1)\n        else:\n            fused_features = visual_flat\n\n        return self.classifier(fused_features)","metadata":{"id":"s6qCd_mGX__e","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:31:47.948831Z","iopub.execute_input":"2025-10-28T10:31:47.949105Z","iopub.status.idle":"2025-10-28T10:31:47.956944Z","shell.execute_reply.started":"2025-10-28T10:31:47.949086Z","shell.execute_reply":"2025-10-28T10:31:47.956352Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"student_model = ClinicalEnhancedStudent(num_classes=num_classes)\nstudent_model = student_model.to(device)\nprint(\"Clinical-enhanced student model initialized\")","metadata":{"id":"4S9kZRxPYEEA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3be4b328-4f8b-4097-df60-f8d92542de49","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:31:53.630181Z","iopub.execute_input":"2025-10-28T10:31:53.630462Z","iopub.status.idle":"2025-10-28T10:31:54.217165Z","shell.execute_reply.started":"2025-10-28T10:31:53.630431Z","shell.execute_reply":"2025-10-28T10:31:54.216366Z"}},"outputs":[{"name":"stdout","text":"Clinical-enhanced student model initialized\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"!pip install --upgrade transformers huggingface-hub\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:52:27.665841Z","iopub.execute_input":"2025-10-28T10:52:27.666619Z","iopub.status.idle":"2025-10-28T10:52:40.464884Z","shell.execute_reply.started":"2025-10-28T10:52:27.666575Z","shell.execute_reply":"2025-10-28T10:52:40.463892Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.41.2)\nCollecting transformers\n  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.23.4)\nCollecting huggingface-hub\n  Downloading huggingface_hub-1.0.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2025.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.23.4\n    Uninstalling huggingface-hub-0.23.4:\n      Successfully uninstalled huggingface-hub-0.23.4\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Uninstalling transformers-4.41.2:\n      Successfully uninstalled transformers-4.41.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface-hub-0.36.0 tokenizers-0.22.1 transformers-4.57.1\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T11:06:09.752384Z","iopub.execute_input":"2025-10-28T11:06:09.752938Z","iopub.status.idle":"2025-10-28T11:06:09.756292Z","shell.execute_reply.started":"2025-10-28T11:06:09.752914Z","shell.execute_reply":"2025-10-28T11:06:09.755498Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"class ClinicalEmbedder:\n    def __init__(self):\n        # Use a more stable model that doesn't have chat template issues\n        model_name = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n        try:\n            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n            self.model = AutoModel.from_pretrained(model_name)\n            self.model.eval()\n            print(f\"Clinical embedder loaded: {model_name}\")\n        except Exception as e:\n            print(f\"Failed to load clinical model: {e}\")\n            print(\"Using fallback embedding method...\")\n            self.model = None\n            self.tokenizer = None\n    \n    def get_embedding(self, text):\n        if self.model is None:\n            # Fallback: return random embeddings of correct dimension\n            return torch.randn(1, 768)\n        \n        try:\n            inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n            with torch.no_grad():\n                outputs = self.model(**inputs)\n            return outputs.last_hidden_state.mean(dim=1)\n        except Exception as e:\n            print(f\" Embedding generation failed: {e}\")\n            return torch.randn(1, 768)\n","metadata":{"id":"iSUo0kSbYJP0","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T11:06:14.095811Z","iopub.execute_input":"2025-10-28T11:06:14.096072Z","iopub.status.idle":"2025-10-28T11:06:14.102175Z","shell.execute_reply.started":"2025-10-28T11:06:14.096053Z","shell.execute_reply":"2025-10-28T11:06:14.101426Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"# Initialize clinical embedder\nclinical_embedder = ClinicalEmbedder()\nprint(\"Clinical embedder initialized\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T11:06:20.094878Z","iopub.execute_input":"2025-10-28T11:06:20.095117Z","iopub.status.idle":"2025-10-28T11:06:20.736308Z","shell.execute_reply.started":"2025-10-28T11:06:20.095102Z","shell.execute_reply":"2025-10-28T11:06:20.735710Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd3d1cb88f18446793b41e9746e865e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecacb82f8153453a9cc3ccc78f7ac71e"}},"metadata":{}},{"name":"stdout","text":"Failed to load clinical model: 404 Client Error. (Request ID: Root=1-6900a3ac-7f5dae1a69c954e32353b54e;be341991-24e6-484b-8f7e-bf651359e9b2)\n\nEntry Not Found for url: https://huggingface.co/api/models/microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract/tree/main/additional_chat_templates?recursive=false&expand=false.\nadditional_chat_templates does not exist on \"main\"\nUsing fallback embedding method...\nClinical embedder initialized\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"# Clinical descriptions for each gait condition\nclinical_descriptions = {\n    \"Normal\": \"Normal symmetrical gait pattern with balanced stride length and cadence\",\n    \"KOA_Early\": \"Early knee osteoarthritis with mild gait modifications and reduced knee flexion\",\n    \"KOA_Mild\": \"Mild knee osteoarthritis showing limping gait and asymmetric weight bearing\",\n    \"KOA_Severe\": \"Severe knee osteoarthritis with significant antalgic gait and reduced mobility\",\n    \"PD_Early\": \"Early Parkinson's disease showing slight shuffling gait and reduced arm swing\",\n    \"PD_Mild\": \"Mild Parkinson's disease with festinating gait and postural instability\",\n    \"PD_Severe\": \"Severe Parkinson's disease showing freezing of gait and significant bradykinesia\",\n    \"Disabled_Assistive\": \"Disabled gait using assistive devices with modified weight distribution\",\n    \"Disabled_NonAssistive\": \"Disabled gait without assistive devices showing compensatory movements\"\n}","metadata":{"id":"51t0XIU3YOvz","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T11:06:28.054280Z","iopub.execute_input":"2025-10-28T11:06:28.054885Z","iopub.status.idle":"2025-10-28T11:06:28.058516Z","shell.execute_reply.started":"2025-10-28T11:06:28.054863Z","shell.execute_reply":"2025-10-28T11:06:28.057931Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"clinical_embedder = ClinicalEmbedder()\nprint(\"Clinical embedder initialized\")","metadata":{"id":"vnz_3pKNYSh5","colab":{"base_uri":"https://localhost:8080/","height":130,"referenced_widgets":["2ce39e8b1c3e4cc2981c217900a1ceda","476a5e23aa8845d4b8a9d399f4992009","5278a447c22c47a7aeeac25dce5d3d0f","1db8c7201585414b8f11caf4f9911daa","62df8a559173415eba014d652763faba","a84b321e06984c93a0e73f1e3d2f191d","316051b72c9a41e08db457027daf7afd","4b42a00b1aa14f96a16c8cc9c5c99658","a6fa2e848d974ebe93aab56de7fd3a80","bcde6503d38d4e73b4aadbd52e6982b7","2b6b7d5809ab4d208c71caf83a7c2bdb","72f43a60afef42648a8cc1b96df041bf","5cabe617ee1f4903bfa7b5fbd9ad6cdb","8d4fa5d5a1ac4c5c806567f984e37a8f","82d00d71fcc3439c91b079510c130c35","196a54c92fd8465fa2587965e071b529","3b3a3aa0f1e64bf99ac6e45a02ed0d83","17dbfa5bc70a4865afd45228b454b49d","ccf80f49c4844e43a9a5df1efffbc2b3","a303c57233cf4edbb682ccf1fff84c11","bd8089195dd44ede930b95ccb17f26e6","be56cf59ca0a4c82a140b12eddf0d7f7","e1c6917cff494266aa95f54e2bbc9771","dce7ecf85e9d410c929ae05e5e22f9ad","98c0dc1b70a1470688b134b8b51fc8cf","5d8d454e02a540cf970c2d10a64b16f4","5a7ba236638944e595d7d034fd785ae0","bdc4ba8871d54b10adafd524a5a3c81d","367ace1e1dfa4e6a9edfb8ffba99caae","e492cf63ae3c4c0a98846eabfbae2ff9","eb5e359c39724be9893b3f9bd6a5cae8","42dcc02b842d4e3cbefd0c39a9efbbde","9d41c1e1510d4e58accaa9e6e65b8d13"]},"outputId":"59a7fd16-592f-448e-d7c3-04f2b7bf04a7","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T11:06:35.287587Z","iopub.execute_input":"2025-10-28T11:06:35.288305Z","iopub.status.idle":"2025-10-28T11:06:35.677304Z","shell.execute_reply.started":"2025-10-28T11:06:35.288283Z","shell.execute_reply":"2025-10-28T11:06:35.676643Z"}},"outputs":[{"name":"stdout","text":"Failed to load clinical model: 404 Client Error. (Request ID: Root=1-6900a3bb-7446ab1d1ea7d2667c08a221;86ca3719-0b2d-4ce0-8ade-3111f3a6c176)\n\nEntry Not Found for url: https://huggingface.co/api/models/microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract/tree/main/additional_chat_templates?recursive=false&expand=false.\nadditional_chat_templates does not exist on \"main\"\nUsing fallback embedding method...\nClinical embedder initialized\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"class KnowledgeDistillationTrainer:\n    def __init__(self, teacher, student, temperature=3.0, alpha=0.7):\n        self.teacher = teacher\n        self.student = student\n        self.temperature = temperature\n        self.alpha = alpha\n        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n        self.ce_loss = nn.CrossEntropyLoss()\n\n    def compute_loss(self, student_logits, teacher_logits, labels, clinical_embeds=None):\n        # Knowledge distillation loss\n        soft_loss = self.kl_loss(\n            F.log_softmax(student_logits / self.temperature, dim=1),\n            F.softmax(teacher_logits / self.temperature, dim=1)\n        ) * (self.temperature ** 2)\n\n        # Classification loss\n        hard_loss = self.ce_loss(student_logits, labels)\n\n        return self.alpha * soft_loss + (1 - self.alpha) * hard_loss, soft_loss, hard_loss","metadata":{"id":"Rqe9cFtSYXj-","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T11:06:40.230837Z","iopub.execute_input":"2025-10-28T11:06:40.231093Z","iopub.status.idle":"2025-10-28T11:06:40.236695Z","shell.execute_reply.started":"2025-10-28T11:06:40.231077Z","shell.execute_reply":"2025-10-28T11:06:40.235780Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"distillation_trainer = KnowledgeDistillationTrainer(teacher_model, student_model)\noptimizer = torch.optim.Adam(student_model.parameters(), lr=1e-4, weight_decay=1e-5)\nprint(\"Distillation trainer and optimizer setup complete\")","metadata":{"id":"GlGtiDa7Ya3E","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e6e277d0-d418-4e16-959d-86d6395872f1","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T11:06:44.333042Z","iopub.execute_input":"2025-10-28T11:06:44.333785Z","iopub.status.idle":"2025-10-28T11:06:44.338718Z","shell.execute_reply.started":"2025-10-28T11:06:44.333761Z","shell.execute_reply":"2025-10-28T11:06:44.337894Z"}},"outputs":[{"name":"stdout","text":"Distillation trainer and optimizer setup complete\n","output_type":"stream"}],"execution_count":75},{"cell_type":"code","source":"def train_epoch():\n    teacher_model.eval()\n    student_model.train()\n\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for batch_idx, (videos, labels) in enumerate(train_loader):\n        videos, labels = videos.to(device), labels.to(device)\n\n        # Get clinical embeddings for this batch\n        clinical_batch = []\n        for label in labels:\n            class_name = list(class_mapping.keys())[list(class_mapping.values()).index(label.item())]\n            desc = clinical_descriptions.get(class_name) # Use .get to handle missing keys\n            if desc:\n                clinical_emb = clinical_embedder.get_embedding(desc).to(device)\n                clinical_batch.append(clinical_emb)\n            else:\n                # Append a zero tensor if description is missing\n                # Assuming clinical embedding size is 768 based on ClinicalEmbedder\n                clinical_batch.append(torch.zeros(1, 768, device=device))\n\n\n        clinical_batch = torch.cat(clinical_batch, dim=0) if clinical_batch else None\n\n        optimizer.zero_grad()\n\n        with torch.no_grad():\n            teacher_outputs = teacher_model(videos)\n\n        student_outputs = student_model(videos, clinical_batch)\n\n        loss, soft_loss, hard_loss = distillation_trainer.compute_loss(\n            student_outputs, teacher_outputs, labels, clinical_batch\n        )\n\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        preds = torch.argmax(student_outputs, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n        if batch_idx % 10 == 0:\n            print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n\n    # Calculate metrics\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n\n    return total_loss / len(train_loader), accuracy, precision, recall, f1","metadata":{"id":"0R8EcMvRYfx-","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T11:06:47.657042Z","iopub.execute_input":"2025-10-28T11:06:47.657525Z","iopub.status.idle":"2025-10-28T11:06:47.665391Z","shell.execute_reply.started":"2025-10-28T11:06:47.657504Z","shell.execute_reply":"2025-10-28T11:06:47.664716Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"def validate():\n    student_model.eval()\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for videos, labels in val_loader:\n            videos, labels = videos.to(device), labels.to(device)\n\n            # Get clinical embeddings\n            clinical_batch = []\n            for label in labels:\n                class_name = list(class_mapping.keys())[list(class_mapping.values()).index(label.item())]\n                desc = clinical_descriptions.get(class_name) # Use .get to handle missing keys\n                if desc:\n                    clinical_emb = clinical_embedder.get_embedding(desc).to(device)\n                    clinical_batch.append(clinical_emb)\n                else:\n                    # Append a zero tensor if description is missing\n                    # Assuming clinical embedding size is 768 based on ClinicalEmbedder\n                    clinical_batch.append(torch.zeros(1, 768, device=device))\n\n\n            clinical_batch = torch.cat(clinical_batch, dim=0) if clinical_batch else None\n            outputs = student_model(videos, clinical_batch)\n            preds = torch.argmax(outputs, dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n\n    class_names = list(class_mapping.keys())\n    # Pass all possible labels to classification_report\n    report = classification_report(all_labels, all_preds, target_names=class_names, zero_division=0, labels=list(range(len(class_mapping))))\n\n    print(\"VALIDATION RESULTS\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1-Score: {f1:.4f}\")\n    print(f\"\\nClassification Report:\\n{report}\")\n\n\n    return accuracy, precision, recall, f1","metadata":{"id":"WZFkNMkNYrp9","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T11:06:52.350413Z","iopub.execute_input":"2025-10-28T11:06:52.351153Z","iopub.status.idle":"2025-10-28T11:06:52.358693Z","shell.execute_reply.started":"2025-10-28T11:06:52.351129Z","shell.execute_reply":"2025-10-28T11:06:52.357820Z"}},"outputs":[],"execution_count":77},{"cell_type":"markdown","source":"## Weighted Sampling for Training\n\n### Subtask:\nImplement weighted random sampling for the training data loader to address class imbalance during training.\n\n**Reasoning**:\nImplement weighted random sampling for the training DataLoader to ensure that batches during training have a more balanced representation of classes. This helps the model learn from minority classes more effectively. Weighted random sampling is typically applied only to the training set to avoid biasing the evaluation metrics on the validation and test sets.","metadata":{"id":"71eb9061"}},{"cell_type":"code","source":"from torch.utils.data import WeightedRandomSampler\nfrom collections import Counter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T11:07:15.062199Z","iopub.execute_input":"2025-10-28T11:07:15.062482Z","iopub.status.idle":"2025-10-28T11:07:15.066277Z","shell.execute_reply.started":"2025-10-28T11:07:15.062457Z","shell.execute_reply":"2025-10-28T11:07:15.065649Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"# Calculate sample weights for the training data\nlabel_counts = Counter(train_labels)\ntotal_samples = len(train_labels)\n\n# Create a list of weights for each sample in the training set\nsample_weights = [1.0 / label_counts[label] for label in train_labels]\n\n# Create a WeightedRandomSampler for the training set\ntrain_sampler = WeightedRandomSampler(\n    weights=sample_weights,\n    num_samples=total_samples, # Sample with replacement for the size of the dataset\n    replacement=True\n)\n\n# Update the train_loader to use the weighted sampler\n# Keep validation and test loaders without sampling (shuffle=False is standard)\nbatch_size = 12\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nprint(f\"Data loaders updated with weighted sampling for training.\")\nprint(f\"Training dataset with augmentation: {len(train_dataset)} videos\")\nprint(f\"Validation dataset: {len(val_dataset)} videos\")\nprint(f\"Test dataset: {len(test_dataset)} videos\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8a26e2d1","outputId":"42056a03-63dc-4d22-c911-2a8e68cb51b0","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T11:07:19.142124Z","iopub.execute_input":"2025-10-28T11:07:19.142915Z","iopub.status.idle":"2025-10-28T11:07:19.149262Z","shell.execute_reply.started":"2025-10-28T11:07:19.142889Z","shell.execute_reply":"2025-10-28T11:07:19.148673Z"}},"outputs":[{"name":"stdout","text":"Data loaders updated with weighted sampling for training.\nTraining dataset with augmentation: 161 videos\nValidation dataset: 23 videos\nTest dataset: 46 videos\n","output_type":"stream"}],"execution_count":80},{"cell_type":"code","source":"distillation_trainer = KnowledgeDistillationTrainer(teacher_model, student_model)\nprint(\"Distillation trainer setup complete\")","metadata":{"id":"85d949ac","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a5bc3acc-1380-47e6-c001-4765feb0ede7","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T11:07:28.017420Z","iopub.execute_input":"2025-10-28T11:07:28.018108Z","iopub.status.idle":"2025-10-28T11:07:28.022279Z","shell.execute_reply.started":"2025-10-28T11:07:28.018085Z","shell.execute_reply":"2025-10-28T11:07:28.021499Z"}},"outputs":[{"name":"stdout","text":"Distillation trainer setup complete\n","output_type":"stream"}],"execution_count":81},{"cell_type":"code","source":"optimizer = torch.optim.Adam(student_model.parameters(), lr=1e-4, weight_decay=1e-5)\nprint(\"Optimizer setup complete\")","metadata":{"id":"c8d282c7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e38501e2-8ce2-4524-c529-a7236f171140","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T11:07:31.604136Z","iopub.execute_input":"2025-10-28T11:07:31.604845Z","iopub.status.idle":"2025-10-28T11:07:31.609107Z","shell.execute_reply.started":"2025-10-28T11:07:31.604819Z","shell.execute_reply":"2025-10-28T11:07:31.608351Z"}},"outputs":[{"name":"stdout","text":"Optimizer setup complete\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"# Training loop\nnum_epochs = 5\n\nprint(\"Starting training...\")\nfor epoch in range(num_epochs):\n    train_loss, train_acc, train_prec, train_rec, train_f1 = train_epoch()\n    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Prec: {train_prec:.4f}, Rec: {train_rec:.4f}, F1: {train_f1:.4f}\")\n\n    # Validation\n    val_acc, val_prec, val_rec, val_f1 = validate()\n    print(f\"Epoch {epoch+1}/{num_epochs} - Val Acc: {val_acc:.4f}, Prec: {val_prec:.4f}, Rec: {val_rec:.4f}, F1: {val_f1:.4f}\")\n\nprint(\"Training and validation complete.\")\n","metadata":{"id":"8ef86762","colab":{"base_uri":"https://localhost:8080/"},"outputId":"50cde1fd-1028-47bd-ea1d-ef8a75ba613b","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T11:07:34.657245Z","iopub.execute_input":"2025-10-28T11:07:34.657873Z","iopub.status.idle":"2025-10-28T11:34:05.009035Z","shell.execute_reply.started":"2025-10-28T11:07:34.657846Z","shell.execute_reply":"2025-10-28T11:34:05.008380Z"}},"outputs":[{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x20de76c0] Input buffer exhausted before END element found\n[aac @ 0x20463940] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Batch 0, Loss: 0.7187\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x24cce400] Input buffer exhausted before END element found\n[aac @ 0x2f7dc340] Input buffer exhausted before END element found\n[aac @ 0x24ccb1c0] Input buffer exhausted before END element found\n[aac @ 0x24d3f100] Input buffer exhausted before END element found\n[aac @ 0x24d3f100] Input buffer exhausted before END element found\n[aac @ 0x2f7dc340] Input buffer exhausted before END element found\n[aac @ 0x2f7d6900] Input buffer exhausted before END element found\n[aac @ 0x24d5cb00] Input buffer exhausted before END element found\n[aac @ 0x2231b840] Input buffer exhausted before END element found\n[aac @ 0x2f7d61c0] Input buffer exhausted before END element found\n[aac @ 0x24d3f100] Input buffer exhausted before END element found\n[aac @ 0x24d3b8c0] Input buffer exhausted before END element found\n[aac @ 0x25442a80] Input buffer exhausted before END element found\n[aac @ 0x2230e9c0] Input buffer exhausted before END element found\n[aac @ 0x24ccddc0] Input buffer exhausted before END element found\n[aac @ 0x25d25cc0] Input buffer exhausted before END element found\n[aac @ 0x2231b840] Input buffer exhausted before END element found\n[aac @ 0x2f7d6900] Input buffer exhausted before END element found\n[aac @ 0x25d18100] Input buffer exhausted before END element found\n[aac @ 0x24cf34c0] Input buffer exhausted before END element found\n[aac @ 0x24ccb1c0] Input buffer exhausted before END element found\n[aac @ 0x24d54380] Input buffer exhausted before END element found\n[aac @ 0x24d3f100] Input buffer exhausted before END element found\n[aac @ 0x24d5cb00] Input buffer exhausted before END element found\n[aac @ 0x24d3f100] Input buffer exhausted before END element found\n[aac @ 0x24cf6bc0] Input buffer exhausted before END element found\n[aac @ 0x24d3bc40] Input buffer exhausted before END element found\n[aac @ 0x24d3bc40] Input buffer exhausted before END element found\n[aac @ 0x2f7d9780] Input buffer exhausted before END element found\n[aac @ 0x24d5cb00] Input buffer exhausted before END element found\n[aac @ 0x2f7d9780] Input buffer exhausted before END element found\n[aac @ 0x2f7d9780] Input buffer exhausted before END element found\n[aac @ 0x2102df00] Input buffer exhausted before END element found\n[aac @ 0x24d3f100] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Batch 10, Loss: 0.6917\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x2f7d1080] Input buffer exhausted before END element found\n[aac @ 0x2f7d1080] Input buffer exhausted before END element found\n[aac @ 0x2c03a7c0] Input buffer exhausted before END element found\n[aac @ 0x24fa4740] Input buffer exhausted before END element found\n[aac @ 0x2f7d1080] Input buffer exhausted before END element found\n[aac @ 0x22fe8b40] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5 - Train Loss: 0.9636, Acc: 0.2857, Prec: 0.2822, Rec: 0.2857, F1: 0.2802\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x22fe9f00] Input buffer exhausted before END element found\n[aac @ 0x2f7d1080] Input buffer exhausted before END element found\n[aac @ 0x24ced4c0] Input buffer exhausted before END element found\n[aac @ 0x2780c240] Input buffer exhausted before END element found\n[aac @ 0x2780c240] Input buffer exhausted before END element found\n[aac @ 0x24d04980] Input buffer exhausted before END element found\n[aac @ 0x2542d280] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"VALIDATION RESULTS\nAccuracy: 0.1739\nPrecision: 0.0717\nRecall: 0.1739\nF1-Score: 0.0975\n\nClassification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.00      0.00      0.00         6\n   Assistive       0.30      1.00      0.46         3\nNonAssistive       0.00      0.00      0.00         4\n     PD_Mild       0.00      0.00      0.00         1\n    PD_Early       0.00      0.00      0.00         1\n   PD_Severe       0.00      0.00      0.00         0\n   KOA_Early       0.25      0.33      0.29         3\n    KOA_Mild       0.00      0.00      0.00         4\n  KOA_Severe       0.00      0.00      0.00         1\n\n    accuracy                           0.17        23\n   macro avg       0.06      0.15      0.08        23\nweighted avg       0.07      0.17      0.10        23\n\nEpoch 1/5 - Val Acc: 0.1739, Prec: 0.0717, Rec: 0.1739, F1: 0.0975\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x2542e2c0] Input buffer exhausted before END element found\n[aac @ 0x2f748180] Input buffer exhausted before END element found\n[aac @ 0x24d1e4c0] Input buffer exhausted before END element found\n[aac @ 0x25a99bc0] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Batch 0, Loss: 0.6540\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x25d162c0] Input buffer exhausted before END element found\n[aac @ 0x2f79d880] Input buffer exhausted before END element found\n[aac @ 0x21027680] Input buffer exhausted before END element found\n[aac @ 0x2f73e540] Input buffer exhausted before END element found\n[aac @ 0x25a96980] Input buffer exhausted before END element found\n[aac @ 0x2c6f8300] Input buffer exhausted before END element found\n[aac @ 0x2f743bc0] Input buffer exhausted before END element found\n[aac @ 0x2f79cc80] Input buffer exhausted before END element found\n[aac @ 0x25a999c0] Input buffer exhausted before END element found\n[aac @ 0x25a999c0] Input buffer exhausted before END element found\n[aac @ 0x2f73fc00] Input buffer exhausted before END element found\n[aac @ 0x2f7a0600] Input buffer exhausted before END element found\n[aac @ 0x25d12500] Input buffer exhausted before END element found\n[aac @ 0x25d12500] Input buffer exhausted before END element found\n[aac @ 0x2f7a0600] Input buffer exhausted before END element found\n[aac @ 0x2542e040] Input buffer exhausted before END element found\n[aac @ 0x25d12500] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x2f748e00] Input buffer exhausted before END element found\n[aac @ 0x2f76ba80] Input buffer exhausted before END element found\n[aac @ 0x2f76ba80] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Batch 10, Loss: 0.6126\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x24d1d680] Input buffer exhausted before END element found\n[aac @ 0x2c925640] Input buffer exhausted before END element found\n[aac @ 0x2f78c440] Input buffer exhausted before END element found\n[aac @ 0x2f73fc00] Input buffer exhausted before END element found\n[aac @ 0x2f79d0c0] Input buffer exhausted before END element found\n[aac @ 0x2c6fe200] Input buffer exhausted before END element found\n[aac @ 0x2f79d0c0] Input buffer exhausted before END element found\n[aac @ 0x2f7a0600] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5 - Train Loss: 0.6143, Acc: 0.4410, Prec: 0.4036, Rec: 0.4410, F1: 0.4040\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x2f76b880] Input buffer exhausted before END element found\n[aac @ 0x2f76b880] Input buffer exhausted before END element found\n[aac @ 0x24d021c0] Input buffer exhausted before END element found\n[aac @ 0x25af7640] Input buffer exhausted before END element found\n[aac @ 0x24d02cc0] Input buffer exhausted before END element found\n[aac @ 0x2f76b880] Input buffer exhausted before END element found\n[aac @ 0x24d02cc0] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"VALIDATION RESULTS\nAccuracy: 0.4348\nPrecision: 0.5290\nRecall: 0.4348\nF1-Score: 0.4478\n\nClassification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.75      0.50      0.60         6\n   Assistive       1.00      0.33      0.50         3\nNonAssistive       0.67      1.00      0.80         4\n     PD_Mild       0.00      0.00      0.00         1\n    PD_Early       0.00      0.00      0.00         1\n   PD_Severe       0.00      0.00      0.00         0\n   KOA_Early       0.00      0.00      0.00         3\n    KOA_Mild       0.50      0.50      0.50         4\n  KOA_Severe       0.00      0.00      0.00         1\n\n    accuracy                           0.43        23\n   macro avg       0.32      0.26      0.27        23\nweighted avg       0.53      0.43      0.45        23\n\nEpoch 2/5 - Val Acc: 0.4348, Prec: 0.5290, Rec: 0.4348, F1: 0.4478\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x30d80b80] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Batch 0, Loss: 0.5793\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x2c7ad540] Input buffer exhausted before END element found\n[aac @ 0x2c7ad540] Input buffer exhausted before END element found\n[aac @ 0x2c7ad540] Input buffer exhausted before END element found\n[aac @ 0x24ddd480] Input buffer exhausted before END element found\n[aac @ 0x2f7416c0] Input buffer exhausted before END element found\n[aac @ 0x2f726fc0] Input buffer exhausted before END element found\n[aac @ 0x2f73a080] Input buffer exhausted before END element found\n[aac @ 0x2f73a040] Input buffer exhausted before END element found\n[mov,mp4,m4a,3gp,3g2,mj2 @ 0x2f73aa80] moov atom not found\n[11:19:14] /github/workspace/src/video/video_reader.cc:83: ERROR opening: /kaggle/input/gaitlabdataset/GiatLabDatset/Normal/015_NM_02.MOV, Invalid data found when processing input\n","output_type":"stream"},{"name":"stdout","text":"Error loading /kaggle/input/gaitlabdataset/GiatLabDatset/Normal/015_NM_02.MOV: Error reading /kaggle/input/gaitlabdataset/GiatLabDatset/Normal/015_NM_02.MOV...\n","output_type":"stream"},{"name":"stderr","text":"[mov,mp4,m4a,3gp,3g2,mj2 @ 0x24d59d40] moov atom not found\n[11:19:31] /github/workspace/src/video/video_reader.cc:83: ERROR opening: /kaggle/input/gaitlabdataset/GiatLabDatset/Normal/015_NM_02.MOV, Invalid data found when processing input\n","output_type":"stream"},{"name":"stdout","text":"Error loading /kaggle/input/gaitlabdataset/GiatLabDatset/Normal/015_NM_02.MOV: Error reading /kaggle/input/gaitlabdataset/GiatLabDatset/Normal/015_NM_02.MOV...\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x2c8d4d80] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x2c7ad440] Input buffer exhausted before END element found\n[aac @ 0x2c7ad440] Input buffer exhausted before END element found\n[aac @ 0x25a954c0] Input buffer exhausted before END element found\n[aac @ 0x2f7416c0] Input buffer exhausted before END element found\n[aac @ 0x2f7416c0] Input buffer exhausted before END element found\n[aac @ 0x24d20e80] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x25ae4e00] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x2c038ec0] Input buffer exhausted before END element found\n[aac @ 0x25d10b40] Input buffer exhausted before END element found\n[aac @ 0x2f79e200] Input buffer exhausted before END element found\n[aac @ 0x2f70acc0] Input buffer exhausted before END element found\n[aac @ 0x2f7416c0] Input buffer exhausted before END element found\n[aac @ 0x24cef280] Input buffer exhausted before END element found\n[aac @ 0x2f7416c0] Input buffer exhausted before END element found\n[aac @ 0x2c0389c0] Input buffer exhausted before END element found\n[aac @ 0x24cef200] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Batch 10, Loss: 0.5498\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x2c6fde80] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5 - Train Loss: 0.5839, Acc: 0.6149, Prec: 0.6513, Rec: 0.6149, F1: 0.6023\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x2102ae00] Input buffer exhausted before END element found\n[aac @ 0x25af8080] Input buffer exhausted before END element found\n[aac @ 0x24d02780] Input buffer exhausted before END element found\n[aac @ 0x2c0389c0] Input buffer exhausted before END element found\n[aac @ 0x24fa3940] Input buffer exhausted before END element found\n[aac @ 0x25a920c0] Input buffer exhausted before END element found\n[aac @ 0x24cef240] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"VALIDATION RESULTS\nAccuracy: 0.6087\nPrecision: 0.6304\nRecall: 0.6087\nF1-Score: 0.6078\n\nClassification Report:\n              precision    recall  f1-score   support\n\n      Normal       1.00      0.83      0.91         6\n   Assistive       0.75      1.00      0.86         3\nNonAssistive       1.00      0.75      0.86         4\n     PD_Mild       1.00      1.00      1.00         1\n    PD_Early       0.50      1.00      0.67         1\n   PD_Severe       0.00      0.00      0.00         0\n   KOA_Early       0.25      0.33      0.29         3\n    KOA_Mild       0.00      0.00      0.00         4\n  KOA_Severe       0.00      0.00      0.00         1\n\n   micro avg       0.61      0.61      0.61        23\n   macro avg       0.50      0.55      0.51        23\nweighted avg       0.63      0.61      0.61        23\n\nEpoch 3/5 - Val Acc: 0.6087, Prec: 0.6304, Rec: 0.6087, F1: 0.6078\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x2102af40] Input buffer exhausted before END element found\n[aac @ 0x2c925d00] Input buffer exhausted before END element found\n[aac @ 0x2f726f80] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Batch 0, Loss: 0.5650\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x25a98340] Input buffer exhausted before END element found\n[aac @ 0x25ac0c40] Input buffer exhausted before END element found\n[aac @ 0x2102ae00] Input buffer exhausted before END element found\n[aac @ 0x24d81e00] Input buffer exhausted before END element found\n[aac @ 0x25a98340] Input buffer exhausted before END element found\n[aac @ 0x2102ae00] Input buffer exhausted before END element found\n[aac @ 0x25af5540] Input buffer exhausted before END element found\n[aac @ 0x25a98340] Input buffer exhausted before END element found\n[aac @ 0x4b47d140] Input buffer exhausted before END element found\n[aac @ 0x2f73f880] Input buffer exhausted before END element found\n[aac @ 0x25a98340] Input buffer exhausted before END element found\n[aac @ 0x25a98340] Input buffer exhausted before END element found\n[aac @ 0x24d0f740] Input buffer exhausted before END element found\n[aac @ 0x25aea500] Input buffer exhausted before END element found\n[aac @ 0x2f738b00] Input buffer exhausted before END element found\n[aac @ 0x25af7840] Input buffer exhausted before END element found\n[aac @ 0x25a98340] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x24d021c0] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x24d02300] Input buffer exhausted before END element found\n[aac @ 0x24d02300] Input buffer exhausted before END element found\n[aac @ 0x25a98340] Input buffer exhausted before END element found\n[aac @ 0x2102ae00] Input buffer exhausted before END element found\n[aac @ 0x25a98340] Input buffer exhausted before END element found\n[aac @ 0x24d02d00] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Batch 10, Loss: 0.5415\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x2f72e5c0] Input buffer exhausted before END element found\n[aac @ 0x24d02d00] Input buffer exhausted before END element found\n[aac @ 0x2c036880] Input buffer exhausted before END element found\n[aac @ 0x25ae3c80] Input buffer exhausted before END element found\n[aac @ 0x25a95580] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5 - Train Loss: 0.5373, Acc: 0.6832, Prec: 0.6928, Rec: 0.6832, F1: 0.6792\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x2102ae00] Input buffer exhausted before END element found\n[aac @ 0x2c03b4c0] Input buffer exhausted before END element found\n[aac @ 0x2102ae00] Input buffer exhausted before END element found\n[aac @ 0x2c03b4c0] Input buffer exhausted before END element found\n[aac @ 0x2102ae00] Input buffer exhausted before END element found\n[aac @ 0x2102ae00] Input buffer exhausted before END element found\n[aac @ 0x2c03b4c0] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"VALIDATION RESULTS\nAccuracy: 0.4348\nPrecision: 0.5543\nRecall: 0.4348\nF1-Score: 0.4373\n\nClassification Report:\n              precision    recall  f1-score   support\n\n      Normal       1.00      0.50      0.67         6\n   Assistive       1.00      0.33      0.50         3\nNonAssistive       0.67      1.00      0.80         4\n     PD_Mild       0.33      1.00      0.50         1\n    PD_Early       0.00      0.00      0.00         1\n   PD_Severe       0.00      0.00      0.00         0\n   KOA_Early       0.25      0.33      0.29         3\n    KOA_Mild       0.00      0.00      0.00         4\n  KOA_Severe       0.00      0.00      0.00         1\n\n   micro avg       0.43      0.43      0.43        23\n   macro avg       0.36      0.35      0.31        23\nweighted avg       0.55      0.43      0.44        23\n\nEpoch 4/5 - Val Acc: 0.4348, Prec: 0.5543, Rec: 0.4348, F1: 0.4373\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x2102ae00] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Batch 0, Loss: 0.5736\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x214b5880] Input buffer exhausted before END element found\n[aac @ 0x2e705340] Input buffer exhausted before END element found\n[aac @ 0x25d12500] Input buffer exhausted before END element found\n[aac @ 0x25af6480] Input buffer exhausted before END element found\n[aac @ 0x2c7ada00] Input buffer exhausted before END element found\n[aac @ 0x24cf6940] Input buffer exhausted before END element found\n[aac @ 0x24cf6940] Input buffer exhausted before END element found\n[aac @ 0x2e705340] Input buffer exhausted before END element found\n[aac @ 0x2102ae00] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x24d02980] Input buffer exhausted before END element found\n[aac @ 0x25af8080] Input buffer exhausted before END element found\n[aac @ 0x2c7aafc0] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x2f7cc080] Input buffer exhausted before END element found\n[aac @ 0x2c03b440] Input buffer exhausted before END element found\n[aac @ 0x25af8080] Input buffer exhausted before END element found\n[aac @ 0x2c7ada00] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x2c7ada00] Input buffer exhausted before END element found\n[aac @ 0x25a95580] Input buffer exhausted before END element found\n[aac @ 0x2542e040] Input buffer exhausted before END element found\n[aac @ 0x2542e040] Input buffer exhausted before END element found\n[aac @ 0x2c8d4c00] Input buffer exhausted before END element found\n[aac @ 0x25a98b00] Input buffer exhausted before END element found\n[aac @ 0x2f76b880] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Batch 10, Loss: 0.4926\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x2f72d5c0] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x2c7ada00] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x214b5880] Input buffer exhausted before END element found\n[aac @ 0x214b5880] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x2f741ac0] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5 - Train Loss: 0.5263, Acc: 0.7764, Prec: 0.8019, Rec: 0.7764, F1: 0.7592\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x2102ae00] Input buffer exhausted before END element found\n[aac @ 0x25af7840] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x2102ae00] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"VALIDATION RESULTS\nAccuracy: 0.4783\nPrecision: 0.5109\nRecall: 0.4783\nF1-Score: 0.4859\n\nClassification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.80      0.67      0.73         6\n   Assistive       1.00      0.67      0.80         3\nNonAssistive       0.80      1.00      0.89         4\n     PD_Mild       0.00      0.00      0.00         1\n    PD_Early       0.00      0.00      0.00         1\n   PD_Severe       0.00      0.00      0.00         0\n   KOA_Early       0.25      0.33      0.29         3\n    KOA_Mild       0.00      0.00      0.00         4\n  KOA_Severe       0.00      0.00      0.00         1\n\n   micro avg       0.48      0.48      0.48        23\n   macro avg       0.32      0.30      0.30        23\nweighted avg       0.51      0.48      0.49        23\n\nEpoch 5/5 - Val Acc: 0.4783, Prec: 0.5109, Rec: 0.4783, F1: 0.4859\nTraining and validation complete.\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"num_epochs = 5\n\nprint(\"Starting training...\")\nfor epoch in range(num_epochs):\n    train_loss, train_acc, train_prec, train_rec, train_f1 = train_epoch()\n    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, Prec: {train_prec:.4f}, Rec: {train_rec:.4f}, F1: {train_f1:.4f}\")\n\n    # Validation\n    val_acc, val_prec, val_rec, val_f1 = validate()\n    print(f\"Epoch {epoch+1}/{num_epochs} - Val Acc: {val_acc:.4f}, Prec: {val_prec:.4f}, Rec: {val_rec:.4f}, F1: {val_f1:.4f}\")\n\nprint(\"Training and validation complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T11:43:02.047148Z","iopub.execute_input":"2025-10-28T11:43:02.047444Z","iopub.status.idle":"2025-10-28T12:09:22.546398Z","shell.execute_reply.started":"2025-10-28T11:43:02.047424Z","shell.execute_reply":"2025-10-28T12:09:22.545606Z"}},"outputs":[{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x25af5d40] Input buffer exhausted before END element found\n[aac @ 0x25af5d40] Input buffer exhausted before END element found\n[aac @ 0x25af5d40] Input buffer exhausted before END element found\n[aac @ 0x25af5d40] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Batch 0, Loss: 0.4647\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x214b5880] Input buffer exhausted before END element found\n[aac @ 0x25af7840] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x25af7840] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x25aead00] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x24cf6900] Input buffer exhausted before END element found\n[aac @ 0x214b5880] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x214b5880] Input buffer exhausted before END element found\n[aac @ 0x25af7840] Input buffer exhausted before END element found\n[aac @ 0x2102ae00] Input buffer exhausted before END element found\n[aac @ 0x2102ae00] Input buffer exhausted before END element found\n[aac @ 0x2f72d980] Input buffer exhausted before END element found\n[aac @ 0x2c7aee80] Input buffer exhausted before END element found\n[aac @ 0x25af5140] Input buffer exhausted before END element found\n[aac @ 0x25af5140] Input buffer exhausted before END element found\n[aac @ 0x25af5140] Input buffer exhausted before END element found\n[aac @ 0x2102ae00] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x25af7840] Input buffer exhausted before END element found\n[aac @ 0x25aa4e00] Input buffer exhausted before END element found\n[aac @ 0x2542e040] Input buffer exhausted before END element found\n[aac @ 0x2f72e140] Input buffer exhausted before END element found\n[aac @ 0x24ddcb40] Input buffer exhausted before END element found\n[aac @ 0x24ddcb40] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x2c7ada00] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Batch 10, Loss: 0.5095\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x2f76b880] Input buffer exhausted before END element found\n[aac @ 0x2542e040] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5 - Train Loss: 0.5094, Acc: 0.7826, Prec: 0.8068, Rec: 0.7826, F1: 0.7676\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x4b47eac0] Input buffer exhausted before END element found\n[aac @ 0x2f76b880] Input buffer exhausted before END element found\n[aac @ 0x21029240] Input buffer exhausted before END element found\n[aac @ 0x2f76b880] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x2f76b880] Input buffer exhausted before END element found\n[aac @ 0x4b47eac0] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"VALIDATION RESULTS\nAccuracy: 0.6087\nPrecision: 0.5693\nRecall: 0.6087\nF1-Score: 0.5805\n\nClassification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.86      1.00      0.92         6\n   Assistive       1.00      0.67      0.80         3\nNonAssistive       0.80      1.00      0.89         4\n     PD_Mild       1.00      1.00      1.00         1\n    PD_Early       0.00      0.00      0.00         1\n   PD_Severe       0.00      0.00      0.00         0\n   KOA_Early       0.25      0.33      0.29         3\n    KOA_Mild       0.00      0.00      0.00         4\n  KOA_Severe       0.00      0.00      0.00         1\n\n   micro avg       0.61      0.61      0.61        23\n   macro avg       0.43      0.44      0.43        23\nweighted avg       0.57      0.61      0.58        23\n\nEpoch 1/5 - Val Acc: 0.6087, Prec: 0.5693, Rec: 0.6087, F1: 0.5805\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x2f76b880] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Batch 0, Loss: 0.4917\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x4b47eac0] Input buffer exhausted before END element found\n[aac @ 0x2f737480] Input buffer exhausted before END element found\n[aac @ 0x24fa4700] Input buffer exhausted before END element found\n[aac @ 0x2f70c600] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x24ceef80] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x25af7840] Input buffer exhausted before END element found\n[aac @ 0x2f722fc0] Input buffer exhausted before END element found\n[aac @ 0x2f722fc0] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x24ceef80] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x24ceef80] Input buffer exhausted before END element found\n[aac @ 0x25aead00] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[mov,mp4,m4a,3gp,3g2,mj2 @ 0x2c03b440] moov atom not found\n[11:51:57] /github/workspace/src/video/video_reader.cc:83: ERROR opening: /kaggle/input/gaitlabdataset/GiatLabDatset/Normal/015_NM_02.MOV, Invalid data found when processing input\n","output_type":"stream"},{"name":"stdout","text":"Error loading /kaggle/input/gaitlabdataset/GiatLabDatset/Normal/015_NM_02.MOV: Error reading /kaggle/input/gaitlabdataset/GiatLabDatset/Normal/015_NM_02.MOV...\nBatch 10, Loss: 0.5368\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x2f76bac0] Input buffer exhausted before END element found\n[aac @ 0x25aead00] Input buffer exhausted before END element found\n[aac @ 0x2c8cf880] Input buffer exhausted before END element found\n[aac @ 0x25671d40] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5 - Train Loss: 0.5223, Acc: 0.8012, Prec: 0.8233, Rec: 0.8012, F1: 0.7918\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x2f76b880] Input buffer exhausted before END element found\n[aac @ 0x2f76b880] Input buffer exhausted before END element found\n[aac @ 0x2542e040] Input buffer exhausted before END element found\n[aac @ 0x2f72d500] Input buffer exhausted before END element found\n[aac @ 0x2102ae00] Input buffer exhausted before END element found\n[aac @ 0x2542e040] Input buffer exhausted before END element found\n[aac @ 0x2542e040] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"VALIDATION RESULTS\nAccuracy: 0.6957\nPrecision: 0.7671\nRecall: 0.6957\nF1-Score: 0.7133\n\nClassification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.86      1.00      0.92         6\n   Assistive       1.00      1.00      1.00         3\nNonAssistive       1.00      1.00      1.00         4\n     PD_Mild       0.00      0.00      0.00         1\n    PD_Early       0.00      0.00      0.00         1\n   PD_Severe       0.00      0.00      0.00         0\n   KOA_Early       0.50      0.33      0.40         3\n    KOA_Mild       1.00      0.50      0.67         4\n  KOA_Severe       0.00      0.00      0.00         1\n\n   micro avg       0.70      0.70      0.70        23\n   macro avg       0.48      0.43      0.44        23\nweighted avg       0.77      0.70      0.71        23\n\nEpoch 2/5 - Val Acc: 0.6957, Prec: 0.7671, Rec: 0.6957, F1: 0.7133\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x24db3780] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Batch 0, Loss: 0.4832\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x25a992c0] Input buffer exhausted before END element found\n[aac @ 0x25a992c0] Input buffer exhausted before END element found\n[aac @ 0x25a992c0] Input buffer exhausted before END element found\n[aac @ 0x25a970c0] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x24d216c0] Input buffer exhausted before END element found\n[aac @ 0x273ec900] Input buffer exhausted before END element found\n[aac @ 0x25671d40] Input buffer exhausted before END element found\n[aac @ 0x2c737c00] Input buffer exhausted before END element found\n[aac @ 0x25a992c0] Input buffer exhausted before END element found\n[aac @ 0x2f735880] Input buffer exhausted before END element found\n[aac @ 0x25671d40] Input buffer exhausted before END element found\n[aac @ 0x25671d40] Input buffer exhausted before END element found\n[aac @ 0x1ca436c0] Input buffer exhausted before END element found\n[aac @ 0x2f79fc40] Input buffer exhausted before END element found\n[aac @ 0x2c737c00] Input buffer exhausted before END element found\n[aac @ 0x25aa4e40] Input buffer exhausted before END element found\n[aac @ 0x2c737c00] Input buffer exhausted before END element found\n[aac @ 0x214b5880] Input buffer exhausted before END element found\n[aac @ 0x24e75980] Input buffer exhausted before END element found\n[aac @ 0x2f72c800] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[mov,mp4,m4a,3gp,3g2,mj2 @ 0x25af32c0] moov atom not found\n[11:57:13] /github/workspace/src/video/video_reader.cc:83: ERROR opening: /kaggle/input/gaitlabdataset/GiatLabDatset/Normal/015_NM_02.MOV, Invalid data found when processing input\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Error loading /kaggle/input/gaitlabdataset/GiatLabDatset/Normal/015_NM_02.MOV: Error reading /kaggle/input/gaitlabdataset/GiatLabDatset/Normal/015_NM_02.MOV...\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x25d24780] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Batch 10, Loss: 0.5070\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x25671d40] Input buffer exhausted before END element found\n[aac @ 0x25af7840] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5 - Train Loss: 0.5206, Acc: 0.7205, Prec: 0.7291, Rec: 0.7205, F1: 0.7016\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x2f736d40] Input buffer exhausted before END element found\n[aac @ 0x25a97c00] Input buffer exhausted before END element found\n[aac @ 0x25a97c00] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"VALIDATION RESULTS\nAccuracy: 0.6522\nPrecision: 0.5975\nRecall: 0.6522\nF1-Score: 0.6212\n\nClassification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.86      1.00      0.92         6\n   Assistive       1.00      1.00      1.00         3\nNonAssistive       1.00      1.00      1.00         4\n     PD_Mild       1.00      1.00      1.00         1\n    PD_Early       0.00      0.00      0.00         1\n   PD_Severe       0.00      0.00      0.00         0\n   KOA_Early       0.20      0.33      0.25         3\n    KOA_Mild       0.00      0.00      0.00         4\n  KOA_Severe       0.00      0.00      0.00         1\n\n   micro avg       0.65      0.65      0.65        23\n   macro avg       0.45      0.48      0.46        23\nweighted avg       0.60      0.65      0.62        23\n\nEpoch 3/5 - Val Acc: 0.6522, Prec: 0.5975, Rec: 0.6522, F1: 0.6212\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x2f72e140] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Batch 0, Loss: 0.5250\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x2f726900] Input buffer exhausted before END element found\n[aac @ 0x2f70fdc0] Input buffer exhausted before END element found\n[aac @ 0x25af6480] Input buffer exhausted before END element found\n[aac @ 0x25ae7d40] Input buffer exhausted before END element found\n[aac @ 0x25aa8ac0] Input buffer exhausted before END element found\n[aac @ 0x25aa8ac0] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x25aa8ac0] Input buffer exhausted before END element found\n[aac @ 0x25aa8ac0] Input buffer exhausted before END element found\n[aac @ 0x25aa8ac0] Input buffer exhausted before END element found\n[aac @ 0x24dad200] Input buffer exhausted before END element found\n[aac @ 0x25d12500] Input buffer exhausted before END element found\n[aac @ 0x25af7840] Input buffer exhausted before END element found\n[aac @ 0x2c6eb5c0] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x21029140] Input buffer exhausted before END element found\n[aac @ 0x25aa6ec0] Input buffer exhausted before END element found\n[aac @ 0x2f736e00] Input buffer exhausted before END element found\n[aac @ 0x25d10b00] Input buffer exhausted before END element found\n[aac @ 0x25aa5540] Input buffer exhausted before END element found\n[aac @ 0x1ca436c0] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Batch 10, Loss: 0.4542\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x24ddd300] Input buffer exhausted before END element found\n[aac @ 0x1ca436c0] Input buffer exhausted before END element found\n[aac @ 0x25d12500] Input buffer exhausted before END element found\n[aac @ 0x2f7416c0] Input buffer exhausted before END element found\n[aac @ 0x25af7840] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5 - Train Loss: 0.4963, Acc: 0.8137, Prec: 0.8091, Rec: 0.8137, F1: 0.8008\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x2f725cc0] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x2f725cc0] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x25af7840] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"VALIDATION RESULTS\nAccuracy: 0.7391\nPrecision: 0.6957\nRecall: 0.7391\nF1-Score: 0.7101\n\nClassification Report:\n              precision    recall  f1-score   support\n\n      Normal       1.00      1.00      1.00         6\n   Assistive       1.00      1.00      1.00         3\nNonAssistive       1.00      1.00      1.00         4\n     PD_Mild       1.00      1.00      1.00         1\n    PD_Early       1.00      1.00      1.00         1\n   PD_Severe       0.00      0.00      0.00         0\n   KOA_Early       0.33      0.67      0.44         3\n    KOA_Mild       0.00      0.00      0.00         4\n  KOA_Severe       0.00      0.00      0.00         1\n\n   micro avg       0.74      0.74      0.74        23\n   macro avg       0.59      0.63      0.60        23\nweighted avg       0.70      0.74      0.71        23\n\nEpoch 4/5 - Val Acc: 0.7391, Prec: 0.6957, Rec: 0.7391, F1: 0.7101\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x25af7840] Input buffer exhausted before END element found\n[aac @ 0x2102ae00] Input buffer exhausted before END element found\n[aac @ 0x25af7840] Input buffer exhausted before END element found\n[aac @ 0x2102ae00] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Batch 0, Loss: 0.4475\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x25c7d7c0] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x25d11100] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[mov,mp4,m4a,3gp,3g2,mj2 @ 0x25d12500] moov atom not found\n[12:05:04] /github/workspace/src/video/video_reader.cc:83: ERROR opening: /kaggle/input/gaitlabdataset/GiatLabDatset/Normal/015_NM_02.MOV, Invalid data found when processing input\n","output_type":"stream"},{"name":"stdout","text":"Error loading /kaggle/input/gaitlabdataset/GiatLabDatset/Normal/015_NM_02.MOV: Error reading /kaggle/input/gaitlabdataset/GiatLabDatset/Normal/015_NM_02.MOV...\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x24fa3fc0] Input buffer exhausted before END element found\n[aac @ 0x24fa3fc0] Input buffer exhausted before END element found\n[aac @ 0x24fa4a00] Input buffer exhausted before END element found\n[aac @ 0x24cf6900] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x25af8480] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x4b47eac0] Input buffer exhausted before END element found\n[aac @ 0x4b47eac0] Input buffer exhausted before END element found\n[aac @ 0x4b47eac0] Input buffer exhausted before END element found\n[aac @ 0x4b47eac0] Input buffer exhausted before END element found\n[aac @ 0x25af8480] Input buffer exhausted before END element found\n[aac @ 0x2f76b880] Input buffer exhausted before END element found\n[aac @ 0x25aa7140] Input buffer exhausted before END element found\n[aac @ 0x4b47eac0] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x25af7840] Input buffer exhausted before END element found\n[aac @ 0x4b47eac0] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Batch 10, Loss: 0.4641\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x2c7370c0] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x25ae8b80] Input buffer exhausted before END element found\n[aac @ 0x25a95740] Input buffer exhausted before END element found\n[mov,mp4,m4a,3gp,3g2,mj2 @ 0x25d12500] moov atom not found\n[12:08:19] /github/workspace/src/video/video_reader.cc:83: ERROR opening: /kaggle/input/gaitlabdataset/GiatLabDatset/Normal/015_NM_02.MOV, Invalid data found when processing input\n","output_type":"stream"},{"name":"stdout","text":"Error loading /kaggle/input/gaitlabdataset/GiatLabDatset/Normal/015_NM_02.MOV: Error reading /kaggle/input/gaitlabdataset/GiatLabDatset/Normal/015_NM_02.MOV...\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x4b47eac0] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x25aeb200] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5 - Train Loss: 0.4880, Acc: 0.8571, Prec: 0.8598, Rec: 0.8571, F1: 0.8475\n","output_type":"stream"},{"name":"stderr","text":"[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x25ae8b80] Input buffer exhausted before END element found\n[aac @ 0x25a95740] Input buffer exhausted before END element found\n[aac @ 0x25a95740] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"VALIDATION RESULTS\nAccuracy: 0.6957\nPrecision: 0.6149\nRecall: 0.6957\nF1-Score: 0.6466\n\nClassification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.86      1.00      0.92         6\n   Assistive       1.00      1.00      1.00         3\nNonAssistive       1.00      1.00      1.00         4\n     PD_Mild       1.00      1.00      1.00         1\n    PD_Early       0.00      0.00      0.00         1\n   PD_Severe       0.00      0.00      0.00         0\n   KOA_Early       0.33      0.67      0.44         3\n    KOA_Mild       0.00      0.00      0.00         4\n  KOA_Severe       0.00      0.00      0.00         1\n\n   micro avg       0.70      0.70      0.70        23\n   macro avg       0.47      0.52      0.49        23\nweighted avg       0.61      0.70      0.65        23\n\nEpoch 5/5 - Val Acc: 0.6957, Prec: 0.6149, Rec: 0.6957, F1: 0.6466\nTraining and validation complete.\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"def test():\n    student_model.eval()\n    all_preds = []\n    all_labels = []\n    all_probabilities = []\n\n    with torch.no_grad():\n        for videos, labels in test_loader:\n            videos, labels = videos.to(device), labels.to(device)\n\n            # Get clinical embeddings\n            clinical_batch = []\n            for label in labels:\n                class_name = list(class_mapping.keys())[list(class_mapping.values()).index(label.item())]\n                desc = clinical_descriptions.get(class_name) # Use .get to handle missing keys\n                if desc:\n                    clinical_emb = clinical_embedder.get_embedding(desc).to(device)\n                    clinical_batch.append(clinical_emb)\n                else:\n                    # Append a zero tensor if description is missing\n                    # Assuming clinical embedding size is 768 based on ClinicalEmbedder\n                    clinical_batch.append(torch.zeros(1, 768, device=device))\n\n\n            clinical_batch = torch.cat(clinical_batch, dim=0) if clinical_batch else None\n            outputs = student_model(videos, clinical_batch)\n            probabilities = F.softmax(outputs, dim=1)\n            preds = torch.argmax(outputs, dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            all_probabilities.extend(probabilities.cpu().numpy())\n\n    # Calculate metrics\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n    recall = recall_score(all_labels, all_labels, average='weighted', zero_division=0) # Corrected recall calculation\n    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n\n    # Confusion matrix\n    cm = confusion_matrix(all_labels, all_preds)\n\n    # Classification report\n    class_names = list(class_mapping.keys())\n    # Pass all possible labels to classification_report\n    report = classification_report(all_labels, all_preds, target_names=class_names, zero_division=0, labels=list(range(len(class_mapping))))\n\n\n    print(\"TEST RESULTS\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1-Score: {f1:.4f}\")\n    print(f\"\\nConfusion Matrix:\\n{cm}\")\n    print(f\"\\nClassification Report:\\n{report}\")\n\n    return accuracy, precision, recall, f1, cm, all_probabilities\n\nprint(\"Testing function defined - ready for final evaluation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T12:10:51.736067Z","iopub.execute_input":"2025-10-28T12:10:51.736354Z","iopub.status.idle":"2025-10-28T12:10:51.745783Z","shell.execute_reply.started":"2025-10-28T12:10:51.736332Z","shell.execute_reply":"2025-10-28T12:10:51.745124Z"}},"outputs":[{"name":"stdout","text":"Testing function defined - ready for final evaluation\n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"test()","metadata":{"id":"JDQCnTaagogS","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T12:10:58.748135Z","iopub.execute_input":"2025-10-28T12:10:58.748405Z","iopub.status.idle":"2025-10-28T12:11:55.353841Z","shell.execute_reply.started":"2025-10-28T12:10:58.748386Z","shell.execute_reply":"2025-10-28T12:11:55.353117Z"}},"outputs":[{"name":"stderr","text":"[aac @ 0x25af6480] Input buffer exhausted before END element found\n[aac @ 0x25af7840] Input buffer exhausted before END element found\n[aac @ 0x24dabac0] Input buffer exhausted before END element found\n[aac @ 0x25aa6c40] Input buffer exhausted before END element found\n[aac @ 0x25aa6c40] Input buffer exhausted before END element found\n[aac @ 0x2f7219c0] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x25af7840] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x24ceff80] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n[aac @ 0x25d24780] Input buffer exhausted before END element found\n","output_type":"stream"},{"name":"stdout","text":"TEST RESULTS\nAccuracy: 0.6957\nPrecision: 0.6913\nRecall: 1.0000\nF1-Score: 0.6768\n\nConfusion Matrix:\n[[11  0  0  0  1  0  0  0  0]\n [ 0  5  0  0  0  0  0  0  0]\n [ 0  0  8  0  0  0  0  0  0]\n [ 2  0  0  1  0  0  0  0  0]\n [ 2  0  0  0  0  1  0  0  0]\n [ 0  0  0  0  0  0  0  0  0]\n [ 0  0  0  0  0  0  4  2  0]\n [ 0  0  0  0  0  0  4  3  1]\n [ 0  0  0  0  0  0  0  1  0]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n      Normal       0.73      0.92      0.81        12\n   Assistive       1.00      1.00      1.00         5\nNonAssistive       1.00      1.00      1.00         8\n     PD_Mild       1.00      0.33      0.50         3\n    PD_Early       0.00      0.00      0.00         3\n   PD_Severe       0.00      0.00      0.00         0\n   KOA_Early       0.50      0.67      0.57         6\n    KOA_Mild       0.50      0.38      0.43         8\n  KOA_Severe       0.00      0.00      0.00         1\n\n    accuracy                           0.70        46\n   macro avg       0.53      0.48      0.48        46\nweighted avg       0.69      0.70      0.68        46\n\n","output_type":"stream"},{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"(0.6956521739130435,\n 0.6913043478260869,\n 1.0,\n 0.6768461007591443,\n array([[11,  0,  0,  0,  1,  0,  0,  0,  0],\n        [ 0,  5,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  0,  8,  0,  0,  0,  0,  0,  0],\n        [ 2,  0,  0,  1,  0,  0,  0,  0,  0],\n        [ 2,  0,  0,  0,  0,  1,  0,  0,  0],\n        [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  0,  0,  0,  0,  0,  4,  2,  0],\n        [ 0,  0,  0,  0,  0,  0,  4,  3,  1],\n        [ 0,  0,  0,  0,  0,  0,  0,  1,  0]]),\n [array([0.06266162, 0.08269726, 0.45133704, 0.05008547, 0.08282047,\n         0.0586655 , 0.05075288, 0.08210236, 0.0788774 ], dtype=float32),\n  array([0.12175764, 0.08469369, 0.07969507, 0.2161927 , 0.13128613,\n         0.10403188, 0.07667983, 0.09298921, 0.09267383], dtype=float32),\n  array([0.22710061, 0.063676  , 0.08536635, 0.1270154 , 0.13825499,\n         0.10287557, 0.08026368, 0.0901201 , 0.08532731], dtype=float32),\n  array([0.22539927, 0.06776547, 0.08220093, 0.10919493, 0.17442921,\n         0.11134362, 0.05855936, 0.07259024, 0.09851692], dtype=float32),\n  array([0.2829013 , 0.06910098, 0.08477573, 0.09663102, 0.14673015,\n         0.10808694, 0.0522787 , 0.07345437, 0.08604083], dtype=float32),\n  array([0.09029213, 0.07048997, 0.07958139, 0.15290755, 0.15976307,\n         0.19136636, 0.08610774, 0.06506687, 0.10442492], dtype=float32),\n  array([0.09460086, 0.09545224, 0.08816385, 0.07219648, 0.06124518,\n         0.09695587, 0.19607598, 0.2108837 , 0.08442574], dtype=float32),\n  array([0.06523143, 0.09049716, 0.42791066, 0.0496451 , 0.08721168,\n         0.06257796, 0.05813587, 0.08530631, 0.07348386], dtype=float32),\n  array([0.09076254, 0.09876621, 0.09658202, 0.06996905, 0.06877661,\n         0.08234386, 0.18903586, 0.22264616, 0.08111773], dtype=float32),\n  array([0.27769127, 0.08808497, 0.05794114, 0.09133676, 0.08569746,\n         0.11010626, 0.07225101, 0.09859496, 0.11829615], dtype=float32),\n  array([0.19398879, 0.09357493, 0.08503506, 0.15632153, 0.14107217,\n         0.08576759, 0.06686317, 0.08994264, 0.0874341 ], dtype=float32),\n  array([0.2003391 , 0.07177632, 0.08612283, 0.14187211, 0.11247253,\n         0.11085178, 0.08356186, 0.10291363, 0.0900899 ], dtype=float32),\n  array([0.08919668, 0.08827864, 0.0947914 , 0.0737591 , 0.07527692,\n         0.0887472 , 0.24317534, 0.16510612, 0.08166854], dtype=float32),\n  array([0.06550828, 0.44848526, 0.07642212, 0.07439951, 0.06314398,\n         0.06394805, 0.07448041, 0.06719401, 0.0664184 ], dtype=float32),\n  array([0.08864932, 0.10025321, 0.09573252, 0.08008473, 0.07172037,\n         0.09299837, 0.17546445, 0.17610256, 0.11899448], dtype=float32),\n  array([0.06646191, 0.11503762, 0.3664397 , 0.05010552, 0.10460047,\n         0.06905069, 0.07238823, 0.08504198, 0.07087392], dtype=float32),\n  array([0.07737544, 0.07572874, 0.09447648, 0.06014778, 0.07878768,\n         0.11449903, 0.25987658, 0.16158985, 0.07751842], dtype=float32),\n  array([0.06876821, 0.39005414, 0.11978041, 0.06882931, 0.07427897,\n         0.06586742, 0.07980336, 0.06675011, 0.0658681 ], dtype=float32),\n  array([0.358233  , 0.07129836, 0.06531461, 0.07727011, 0.08424923,\n         0.11786078, 0.06092449, 0.08897325, 0.07587617], dtype=float32),\n  array([0.08340032, 0.09657665, 0.3366382 , 0.0630514 , 0.12023407,\n         0.06359544, 0.0681155 , 0.09145827, 0.07693017], dtype=float32),\n  array([0.22685228, 0.07366318, 0.08135501, 0.10770307, 0.18586473,\n         0.12334432, 0.05606436, 0.05881693, 0.08633611], dtype=float32),\n  array([0.06238041, 0.46088144, 0.07547624, 0.07389715, 0.06064474,\n         0.06017141, 0.07481354, 0.06885248, 0.06288265], dtype=float32),\n  array([0.09047332, 0.11089203, 0.08880244, 0.08087349, 0.06257862,\n         0.0862252 , 0.16234475, 0.2207277 , 0.09708247], dtype=float32),\n  array([0.07375382, 0.06451632, 0.09314603, 0.05926708, 0.1060072 ,\n         0.11237865, 0.2684596 , 0.13333523, 0.08913609], dtype=float32),\n  array([0.08807961, 0.07700203, 0.08794629, 0.06214507, 0.08312631,\n         0.10414641, 0.2466668 , 0.15471198, 0.09617553], dtype=float32),\n  array([0.15078975, 0.06303082, 0.09154367, 0.15233853, 0.16002017,\n         0.11855263, 0.09348538, 0.08977579, 0.08046327], dtype=float32),\n  array([0.09152932, 0.07768574, 0.09213248, 0.0538752 , 0.07623511,\n         0.11159927, 0.25768384, 0.15871668, 0.08054229], dtype=float32),\n  array([0.0742168 , 0.17429377, 0.25583538, 0.06772711, 0.09568331,\n         0.06259251, 0.07451002, 0.10196909, 0.093172  ], dtype=float32),\n  array([0.06338971, 0.45066726, 0.07793578, 0.0728687 , 0.06169668,\n         0.06141653, 0.07808466, 0.07111298, 0.06282765], dtype=float32),\n  array([0.08297254, 0.07651751, 0.0858046 , 0.06381856, 0.11442936,\n         0.08649755, 0.26583025, 0.13453518, 0.08959446], dtype=float32),\n  array([0.3133433 , 0.08366065, 0.05811065, 0.07516573, 0.07944443,\n         0.11369367, 0.06383928, 0.10079519, 0.1119471 ], dtype=float32),\n  array([0.24510331, 0.06184151, 0.08800714, 0.09510034, 0.20001857,\n         0.09247531, 0.05796745, 0.0653587 , 0.09412769], dtype=float32),\n  array([0.24671717, 0.06980533, 0.07575519, 0.10387075, 0.1376893 ,\n         0.10435073, 0.0811896 , 0.0898338 , 0.09078807], dtype=float32),\n  array([0.28506696, 0.07766794, 0.0663228 , 0.08629646, 0.08649962,\n         0.10869486, 0.06092478, 0.11803356, 0.11049306], dtype=float32),\n  array([0.08798639, 0.11761693, 0.0896661 , 0.07852128, 0.06286819,\n         0.08018468, 0.15749857, 0.21608664, 0.10957129], dtype=float32),\n  array([0.07711831, 0.372047  , 0.11083699, 0.06916969, 0.0750536 ,\n         0.0700771 , 0.08476336, 0.07098727, 0.06994665], dtype=float32),\n  array([0.349061  , 0.07261649, 0.06663629, 0.07662153, 0.11596575,\n         0.10653619, 0.05502916, 0.08523723, 0.07229635], dtype=float32),\n  array([0.07847856, 0.08392854, 0.09578276, 0.08872677, 0.08005449,\n         0.09658328, 0.13199112, 0.15474775, 0.18970671], dtype=float32),\n  array([0.07635614, 0.07829086, 0.09158432, 0.05898268, 0.09476724,\n         0.11063553, 0.25917238, 0.15302333, 0.07718747], dtype=float32),\n  array([0.0846937 , 0.11774475, 0.30927143, 0.07076145, 0.11017104,\n         0.07717799, 0.06793616, 0.08685841, 0.07538506], dtype=float32),\n  array([0.0801595 , 0.11554623, 0.32160628, 0.06781227, 0.11619367,\n         0.06531758, 0.06326424, 0.0864558 , 0.08364438], dtype=float32),\n  array([0.07319831, 0.13969931, 0.30875346, 0.06293516, 0.10429008,\n         0.06104066, 0.07384196, 0.09571512, 0.08052596], dtype=float32),\n  array([0.08044507, 0.07324596, 0.09203694, 0.06346159, 0.11269471,\n         0.0936066 , 0.26435485, 0.13902543, 0.08112876], dtype=float32),\n  array([0.32519943, 0.07654567, 0.05546059, 0.06393104, 0.09566462,\n         0.10651437, 0.06722324, 0.10110684, 0.10835422], dtype=float32),\n  array([0.11700365, 0.07968704, 0.09135529, 0.06812096, 0.0646221 ,\n         0.11040685, 0.1711951 , 0.21936764, 0.07824133], dtype=float32),\n  array([0.18093012, 0.0741846 , 0.08083869, 0.1508245 , 0.12119014,\n         0.10855597, 0.09030431, 0.09740599, 0.09576569], dtype=float32)])"},"metadata":{}}],"execution_count":87},{"cell_type":"code","source":"def export_model(model, filename=\"student_model.pth\"):\n    \"\"\"Saves the trained student model.\"\"\"\n    torch.save(model.state_dict(), filename)\n    print(f\"Model saved to {filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T12:14:32.336980Z","iopub.execute_input":"2025-10-28T12:14:32.337740Z","iopub.status.idle":"2025-10-28T12:14:32.341859Z","shell.execute_reply.started":"2025-10-28T12:14:32.337709Z","shell.execute_reply":"2025-10-28T12:14:32.341027Z"}},"outputs":[],"execution_count":88},{"cell_type":"code","source":"# Export the trained student model\nexport_model(student_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T12:15:11.535056Z","iopub.execute_input":"2025-10-28T12:15:11.535521Z","iopub.status.idle":"2025-10-28T12:15:11.609974Z","shell.execute_reply.started":"2025-10-28T12:15:11.535501Z","shell.execute_reply":"2025-10-28T12:15:11.609300Z"}},"outputs":[{"name":"stdout","text":"Model saved to student_model.pth\n","output_type":"stream"}],"execution_count":89},{"cell_type":"code","source":"def infer_with_model(video_input, clinical_description, model_path, clinical_embedder, class_mapping, num_frames=16, frame_size=224, device='cuda'):\n    \"\"\"\n    Performs inference on a single video input (path or data) and clinical description using the student model.\n\n    Args:\n        video_input (str or np.ndarray or torch.Tensor): Path to the video file or the video data directly.\n                                                          If video data is provided, it should be a NumPy array\n                                                          or a PyTorch tensor with shape (frames, height, width, channels)\n                                                          or (channels, frames, height, width).\n        clinical_description (str): Text description of the clinical condition.\n        model_path (str): Path to the saved student model state dictionary (.pth file).\n        clinical_embedder (ClinicalEmbedder): The clinical embedder model.\n        class_mapping (dict): Dictionary mapping class names to indices.\n        num_frames (int): Number of frames to sample from the video (only used if video_input is a path).\n        frame_size (int): Size to resize frames to.\n        device (str): Device to run inference on ('cuda' or 'cpu').\n\n    Returns:\n        tuple: A tuple containing:\n            - logits (torch.Tensor): Raw output logits from the model.\n            - probabilities (torch.Tensor): Softmax probabilities over classes.\n            - predicted_class_index (int): Index of the predicted class.\n            - predicted_class_name (str): Name of the predicted class.\n    \"\"\"\n    # Initialize the student model architecture\n    num_classes = len(class_mapping)\n    student_model = ClinicalEnhancedStudent(num_classes=num_classes)\n    student_model.to(device)\n\n    # Load the saved model state dictionary\n    try:\n        student_model.load_state_dict(torch.load(model_path, map_location=device))\n        print(f\"Student model loaded successfully from {model_path}\")\n    except FileNotFoundError:\n        print(f\"Error: Model file not found at {model_path}\")\n        return None, None, None, \"Error: Model not loaded\"\n    except Exception as e:\n        print(f\"Error loading model from {model_path}: {e}\")\n        return None, None, None, \"Error: Model not loaded\"\n\n\n    student_model.eval()\n\n    try:\n        if isinstance(video_input, str):\n            # Process video from path\n            vr = VideoReader(video_input, ctx=cpu(0))\n            total_frames = len(vr)\n\n            if total_frames <= num_frames:\n                frame_indices = list(range(total_frames))\n                while len(frame_indices) < num_frames:\n                    frame_indices.append(frame_indices[-1])\n            else:\n                frame_indices = np.linspace(0, total_frames-1, num_frames, dtype=int)\n\n            frames = vr.get_batch(frame_indices).asnumpy()\n\n            # Resize frames\n            resized_frames = []\n            for frame in frames:\n                resized_frame = cv2.resize(frame, (frame_size, frame_size))\n                resized_frames.append(resized_frame)\n            frames = np.array(resized_frames)\n\n            # Convert to tensor and normalize\n            videos = torch.from_numpy(frames).permute(3, 0, 1, 2).float()\n            if videos.max() > 1.0:\n                videos = videos / 255.0\n\n            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1, 1)\n            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1, 1)\n            videos = (videos - mean) / std\n\n        elif isinstance(video_input, (np.ndarray, torch.Tensor)):\n            # Process video from data\n            videos = video_input\n            if isinstance(videos, np.ndarray):\n                videos = torch.from_numpy(videos).float()\n\n            # Ensure the tensor has the correct shape [channels, frames, height, width]\n            # Assume input is either [frames, height, width, channels] or [channels, frames, height, width]\n            if videos.ndim == 4:\n                if videos.shape[-1] == 3: # Assuming last dim is channel if size is 3\n                    videos = videos.permute(3, 0, 1, 2) # Convert from [frames, h, w, c] to [c, frames, h, w]\n                elif videos.shape[0] != 3: # Assuming first dim is channel if not frames\n                     # This case is ambiguous, might need more specific checks or require a fixed input format\n                     # For now, assume it's already [c, frames, h, w] if first dim is not 3 but ndim is 4\n                     pass # Already in [c, frames, h, w] format\n\n            # Resize frames if necessary (assuming the input data might not be the target size)\n            current_frame_size = videos.shape[2] # Assuming shape is [c, frames, h, w]\n            if current_frame_size != frame_size:\n                 # Need to resize each frame. This is more complex for a batched tensor.\n                 # For simplicity, let's assume the input video data is already pre-processed\n                 # to the correct frame_size. If not, this would require iterating or using\n                 # torchvision.transforms.functional.resize, which might not be ideal for 4D tensors.\n                 # Add a warning or error if resizing is needed but not implemented for tensor input.\n                 if current_frame_size != frame_size:\n                     print(f\"Warning: Input video data has frame size {current_frame_size}, expected {frame_size}. Resizing not implemented for tensor input.\")\n\n\n            # Normalize if necessary (check max value as a simple heuristic)\n            if videos.max() > 1.0:\n                videos = videos / 255.0\n\n            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1, 1)\n            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1, 1)\n            videos = (videos - mean) / std\n\n        else:\n            raise TypeError(\"video_input must be a file path (str) or video data (np.ndarray or torch.Tensor)\")\n\n\n        videos = videos.unsqueeze(0).to(device) # Add batch dimension and move to device\n\n        # Get clinical embedding\n        clinical_embeds = clinical_embedder.get_embedding(clinical_description).to(device)\n\n\n        # Perform inference\n        with torch.no_grad():\n            logits = student_model(videos, clinical_embeds)\n            probabilities = F.softmax(logits, dim=1)\n            predicted_class_index = torch.argmax(logits, dim=1).item()\n\n        # Get predicted class name\n        idx_to_class = {v: k for k, v in class_mapping.items()}\n        predicted_class_name = idx_to_class.get(predicted_class_index, \"Unknown\")\n\n        return logits, probabilities, predicted_class_index, predicted_class_name\n\n    except Exception as e:\n        print(f\"Error during inference: {e}\")\n        return None, None, None, \"Error\"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}